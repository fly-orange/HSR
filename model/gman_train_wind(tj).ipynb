{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchsummary import *\n",
    "from gman3 import *\n",
    "import time\n",
    "import datetime\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log string\n",
    "def log_string(log, string):\n",
    "    log.write(string + '\\n')\n",
    "    log.flush()\n",
    "    print(string)\n",
    "# metric\n",
    "def metric(pred, label):\n",
    "    mask = torch.ne(label, 0)\n",
    "    mask = mask.type(torch.float32)\n",
    "    mask /= torch.mean(mask)\n",
    "    mae = torch.abs(torch.sub(pred, label)).type(torch.float32)\n",
    "    rmse = mae ** 2\n",
    "    mape = mae / label\n",
    "    mae = torch.mean(mae)\n",
    "    rmse = rmse * mask\n",
    "    rmse = torch.sqrt(torch.mean(rmse))\n",
    "    mape = mape * mask\n",
    "    mape = torch.mean(mape)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "def seq2instance(data, P, Q):\n",
    "    num_step, dims = data.shape\n",
    "    num_sample = num_step - P - Q + 1\n",
    "    x = torch.zeros(num_sample, P, dims)\n",
    "    y = torch.zeros(num_sample, Q, dims)\n",
    "    for i in range(num_sample):\n",
    "        x[i] = data[i : i + P]\n",
    "        y[i] = data[i + P : i + P + Q]\n",
    "    return x, y\n",
    "\n",
    "def loadData(data_args):\n",
    "    # Traffic\n",
    "    df = pd.read_csv(data_args['data_file'],index_col=0)\n",
    "    Traffic = torch.from_numpy(df.values)\n",
    "    # train/val/test \n",
    "    num_step = df.shape[0]\n",
    "    train_steps = round(data_args['train_ratio'] * num_step)\n",
    "    test_steps = round(data_args['test_ratio'] * num_step)\n",
    "    val_steps = num_step - train_steps - test_steps\n",
    "    train = Traffic[: train_steps]\n",
    "    val = Traffic[train_steps : train_steps + val_steps]\n",
    "    test = Traffic[-test_steps :]\n",
    "    # X, Y \n",
    "    trainX, trainY = seq2instance(train, data_args['P'], data_args['Q'])\n",
    "    valX, valY = seq2instance(val,data_args['P'], data_args['Q'])\n",
    "    testX, testY = seq2instance(test,data_args['P'], data_args['Q'])\n",
    "    # normalization\n",
    "    mean, std = torch.mean(trainX), torch.std(trainX)\n",
    "    trainX = (trainX - mean) / std\n",
    "    valX = (valX - mean) / std\n",
    "    testX = (testX - mean) / std\n",
    "    \n",
    "    # temporal embedding \n",
    "    time = pd.DatetimeIndex(df.index)\n",
    "    dayofweek = torch.reshape(torch.tensor(time.weekday), (-1, 1))\n",
    "#     timeofday = (time.hour * 3600 + time.minute * 60 + time.second) \\\n",
    "#                 // time.freq.delta.total_seconds()\n",
    "#     timeofday = torch.reshape(torch.tensor(timeofday), (-1, 1))\n",
    "    timeofday = (time.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
    "    timeofday = torch.reshape(torch.tensor(timeofday), (-1, 1))        \n",
    "    time = torch.cat((dayofweek, timeofday), -1)\n",
    "    # train/val/test\n",
    "    train = time[: train_steps]\n",
    "    val = time[train_steps : train_steps + val_steps]\n",
    "    test = time[-test_steps :]\n",
    "    # shape = (num_sample, P + Q, 2)\n",
    "    trainTE = seq2instance(train,data_args['P'], data_args['Q'])\n",
    "    trainTE = torch.cat(trainTE, 1).type(torch.int32)\n",
    "    valTE = seq2instance(val, data_args['P'], data_args['Q'])\n",
    "    valTE = torch.cat(valTE, 1).type(torch.int32)\n",
    "    testTE = seq2instance(test, data_args['P'],data_args['Q'])\n",
    "    testTE = torch.cat(testTE, 1).type(torch.int32)\n",
    "    \n",
    "    return (trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY,\n",
    "             mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_args={\n",
    "'data_file' : './Dataset/Wind/tj_wind_5m.csv',\n",
    "# 'SE_file' : './Dataset/PEMS-BAY/SE(PeMS).txt',\n",
    "'train_ratio' : 0.7,\n",
    "'test_ratio' : 0.1,\n",
    "'P' : 36,\n",
    "'Q' : 12\n",
    "}\n",
    "(trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY,\n",
    " mean, std) = loadData(data_args)\n",
    "SE = torch.zeros((12,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73537, 36, 12])\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnodes = 12\n",
    "ne = 3\n",
    "ndim = 32\n",
    "alpha = 0.5\n",
    "L = 3\n",
    "K = 8\n",
    "d = 8\n",
    "num_his = data_args['P']\n",
    "num_pred = data_args['Q']\n",
    "bn_decay =0.1\n",
    "steps_per_day = 288\n",
    "use_bias = False\n",
    "mask = True\n",
    "model = GMAN4(nnodes,ne,ndim,alpha,L,K,d,num_his,bn_decay,steps_per_day,use_bias,mask,SE,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torchsummary import *\n",
    "# summary(model,input_size=[(12,325),(15,2)],batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs =100\n",
    "patience =7\n",
    "batch_size = 128\n",
    "LR = 0.01\n",
    "decay_epoch = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                      step_size=decay_epoch,\n",
    "                                      gamma=0.9)\n",
    "loss_criterion = nn.L1Loss().to(device)\n",
    "model_file = './Model/GMAN/gman4_tjwind(1).pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** training model ****\n",
      "Training batch: 5 in epoch:0, training batch loss:0.6573\n",
      "Training batch: 10 in epoch:0, training batch loss:0.5978\n",
      "Training batch: 15 in epoch:0, training batch loss:0.5595\n",
      "Training batch: 20 in epoch:0, training batch loss:0.5408\n",
      "Training batch: 25 in epoch:0, training batch loss:0.5232\n",
      "Training batch: 30 in epoch:0, training batch loss:0.5104\n",
      "Training batch: 35 in epoch:0, training batch loss:0.5114\n",
      "Training batch: 40 in epoch:0, training batch loss:0.4926\n",
      "Training batch: 45 in epoch:0, training batch loss:0.4850\n",
      "Training batch: 50 in epoch:0, training batch loss:0.4940\n",
      "Training batch: 55 in epoch:0, training batch loss:0.4861\n",
      "Training batch: 60 in epoch:0, training batch loss:0.4799\n",
      "Training batch: 65 in epoch:0, training batch loss:0.4587\n",
      "Training batch: 70 in epoch:0, training batch loss:0.4787\n",
      "Training batch: 75 in epoch:0, training batch loss:0.4686\n",
      "Training batch: 80 in epoch:0, training batch loss:0.4789\n",
      "Training batch: 85 in epoch:0, training batch loss:0.4664\n",
      "Training batch: 90 in epoch:0, training batch loss:0.4691\n",
      "Training batch: 95 in epoch:0, training batch loss:0.4759\n",
      "Training batch: 100 in epoch:0, training batch loss:0.4681\n",
      "Training batch: 105 in epoch:0, training batch loss:0.4711\n",
      "Training batch: 110 in epoch:0, training batch loss:0.4510\n",
      "Training batch: 115 in epoch:0, training batch loss:0.4587\n",
      "Training batch: 120 in epoch:0, training batch loss:0.4602\n",
      "Training batch: 125 in epoch:0, training batch loss:0.4599\n",
      "Training batch: 130 in epoch:0, training batch loss:0.4674\n",
      "Training batch: 135 in epoch:0, training batch loss:0.4507\n",
      "Training batch: 140 in epoch:0, training batch loss:0.4470\n",
      "Training batch: 145 in epoch:0, training batch loss:0.4650\n",
      "Training batch: 150 in epoch:0, training batch loss:0.4525\n",
      "Training batch: 155 in epoch:0, training batch loss:0.4532\n",
      "Training batch: 160 in epoch:0, training batch loss:0.4535\n",
      "Training batch: 165 in epoch:0, training batch loss:0.4543\n",
      "Training batch: 170 in epoch:0, training batch loss:0.4508\n",
      "Training batch: 175 in epoch:0, training batch loss:0.4481\n",
      "Training batch: 180 in epoch:0, training batch loss:0.4495\n",
      "Training batch: 185 in epoch:0, training batch loss:0.4456\n",
      "Training batch: 190 in epoch:0, training batch loss:0.4580\n",
      "Training batch: 195 in epoch:0, training batch loss:0.4476\n",
      "Training batch: 200 in epoch:0, training batch loss:0.4567\n",
      "Training batch: 205 in epoch:0, training batch loss:0.4690\n",
      "Training batch: 210 in epoch:0, training batch loss:0.4450\n",
      "Training batch: 215 in epoch:0, training batch loss:0.4521\n",
      "Training batch: 220 in epoch:0, training batch loss:0.4405\n",
      "Training batch: 225 in epoch:0, training batch loss:0.4595\n",
      "Training batch: 230 in epoch:0, training batch loss:0.4578\n",
      "Training batch: 235 in epoch:0, training batch loss:0.4562\n",
      "Training batch: 240 in epoch:0, training batch loss:0.4565\n",
      "Training batch: 245 in epoch:0, training batch loss:0.4454\n",
      "Training batch: 250 in epoch:0, training batch loss:0.4392\n",
      "Training batch: 255 in epoch:0, training batch loss:0.4391\n",
      "Training batch: 260 in epoch:0, training batch loss:0.4421\n",
      "Training batch: 265 in epoch:0, training batch loss:0.4404\n",
      "Training batch: 270 in epoch:0, training batch loss:0.4421\n",
      "Training batch: 275 in epoch:0, training batch loss:0.4568\n",
      "Training batch: 280 in epoch:0, training batch loss:0.4435\n",
      "Training batch: 285 in epoch:0, training batch loss:0.4435\n",
      "Training batch: 290 in epoch:0, training batch loss:0.4525\n",
      "Training batch: 295 in epoch:0, training batch loss:0.4605\n",
      "Training batch: 300 in epoch:0, training batch loss:0.4520\n",
      "Training batch: 305 in epoch:0, training batch loss:0.4494\n",
      "Training batch: 310 in epoch:0, training batch loss:0.4458\n",
      "Training batch: 315 in epoch:0, training batch loss:0.4494\n",
      "Training batch: 320 in epoch:0, training batch loss:0.4611\n",
      "Training batch: 325 in epoch:0, training batch loss:0.4344\n",
      "Training batch: 330 in epoch:0, training batch loss:0.4491\n",
      "Training batch: 335 in epoch:0, training batch loss:0.4396\n",
      "Training batch: 340 in epoch:0, training batch loss:0.4408\n",
      "Training batch: 345 in epoch:0, training batch loss:0.4546\n",
      "Training batch: 350 in epoch:0, training batch loss:0.4356\n",
      "Training batch: 355 in epoch:0, training batch loss:0.4378\n",
      "Training batch: 360 in epoch:0, training batch loss:0.4454\n",
      "Training batch: 365 in epoch:0, training batch loss:0.4494\n",
      "Training batch: 370 in epoch:0, training batch loss:0.4497\n",
      "Training batch: 375 in epoch:0, training batch loss:0.4526\n",
      "Training batch: 380 in epoch:0, training batch loss:0.4320\n",
      "Training batch: 385 in epoch:0, training batch loss:0.4434\n",
      "Training batch: 390 in epoch:0, training batch loss:0.4483\n",
      "Training batch: 395 in epoch:0, training batch loss:0.4366\n",
      "Training batch: 400 in epoch:0, training batch loss:0.4333\n",
      "Training batch: 405 in epoch:0, training batch loss:0.4433\n",
      "Training batch: 410 in epoch:0, training batch loss:0.4406\n",
      "Training batch: 415 in epoch:0, training batch loss:0.4388\n",
      "Training batch: 420 in epoch:0, training batch loss:0.4449\n",
      "Training batch: 425 in epoch:0, training batch loss:0.4527\n",
      "Training batch: 430 in epoch:0, training batch loss:0.4464\n",
      "Training batch: 435 in epoch:0, training batch loss:0.4368\n",
      "Training batch: 440 in epoch:0, training batch loss:0.4490\n",
      "Training batch: 445 in epoch:0, training batch loss:0.4457\n",
      "Training batch: 450 in epoch:0, training batch loss:0.4368\n",
      "Training batch: 455 in epoch:0, training batch loss:0.4425\n",
      "Training batch: 460 in epoch:0, training batch loss:0.4444\n",
      "Training batch: 465 in epoch:0, training batch loss:0.4353\n",
      "Training batch: 470 in epoch:0, training batch loss:0.4524\n",
      "Training batch: 475 in epoch:0, training batch loss:0.4463\n",
      "Training batch: 480 in epoch:0, training batch loss:0.4516\n",
      "Training batch: 485 in epoch:0, training batch loss:0.4460\n",
      "Training batch: 490 in epoch:0, training batch loss:0.4357\n",
      "Training batch: 495 in epoch:0, training batch loss:0.4350\n",
      "Training batch: 500 in epoch:0, training batch loss:0.4371\n",
      "Training batch: 505 in epoch:0, training batch loss:0.4428\n",
      "Training batch: 510 in epoch:0, training batch loss:0.4467\n",
      "Training batch: 515 in epoch:0, training batch loss:0.4264\n",
      "Training batch: 520 in epoch:0, training batch loss:0.4373\n",
      "Training batch: 525 in epoch:0, training batch loss:0.4441\n",
      "Training batch: 530 in epoch:0, training batch loss:0.4440\n",
      "Training batch: 535 in epoch:0, training batch loss:0.4370\n",
      "Training batch: 540 in epoch:0, training batch loss:0.4569\n",
      "Training batch: 545 in epoch:0, training batch loss:0.4370\n",
      "Training batch: 550 in epoch:0, training batch loss:0.4398\n",
      "Training batch: 555 in epoch:0, training batch loss:0.4494\n",
      "Training batch: 560 in epoch:0, training batch loss:0.4445\n",
      "Training batch: 565 in epoch:0, training batch loss:0.4308\n",
      "Training batch: 570 in epoch:0, training batch loss:0.4453\n",
      "Training batch: 575 in epoch:0, training batch loss:0.4407\n",
      "2021-04-21 15:21:52 | epoch: 0001/100, training time: 1228.7s, inference time: 304.8s\n",
      "train loss: 0.4582, val_loss: 0.4412\n",
      "val loss decrease from inf to 0.4412, saving model to ./Model/GMAN/gman4_tjwind(1).pkl\n",
      "Training batch: 5 in epoch:1, training batch loss:0.4370\n",
      "Training batch: 10 in epoch:1, training batch loss:0.4454\n",
      "Training batch: 15 in epoch:1, training batch loss:0.4384\n",
      "Training batch: 20 in epoch:1, training batch loss:0.4468\n",
      "Training batch: 25 in epoch:1, training batch loss:0.4539\n",
      "Training batch: 30 in epoch:1, training batch loss:0.4316\n",
      "Training batch: 35 in epoch:1, training batch loss:0.4446\n",
      "Training batch: 40 in epoch:1, training batch loss:0.4410\n",
      "Training batch: 45 in epoch:1, training batch loss:0.4320\n",
      "Training batch: 50 in epoch:1, training batch loss:0.4455\n",
      "Training batch: 55 in epoch:1, training batch loss:0.4282\n",
      "Training batch: 60 in epoch:1, training batch loss:0.4457\n",
      "Training batch: 65 in epoch:1, training batch loss:0.4413\n",
      "Training batch: 70 in epoch:1, training batch loss:0.4529\n",
      "Training batch: 75 in epoch:1, training batch loss:0.4266\n",
      "Training batch: 80 in epoch:1, training batch loss:0.4343\n",
      "Training batch: 85 in epoch:1, training batch loss:0.4451\n",
      "Training batch: 90 in epoch:1, training batch loss:0.4324\n",
      "Training batch: 95 in epoch:1, training batch loss:0.4260\n",
      "Training batch: 100 in epoch:1, training batch loss:0.4385\n",
      "Training batch: 105 in epoch:1, training batch loss:0.4476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 110 in epoch:1, training batch loss:0.4385\n",
      "Training batch: 115 in epoch:1, training batch loss:0.4407\n",
      "Training batch: 120 in epoch:1, training batch loss:0.4440\n",
      "Training batch: 125 in epoch:1, training batch loss:0.4339\n",
      "Training batch: 130 in epoch:1, training batch loss:0.4508\n",
      "Training batch: 135 in epoch:1, training batch loss:0.4357\n",
      "Training batch: 140 in epoch:1, training batch loss:0.4322\n",
      "Training batch: 145 in epoch:1, training batch loss:0.4465\n",
      "Training batch: 150 in epoch:1, training batch loss:0.4343\n",
      "Training batch: 155 in epoch:1, training batch loss:0.4351\n",
      "Training batch: 160 in epoch:1, training batch loss:0.4519\n",
      "Training batch: 165 in epoch:1, training batch loss:0.4435\n",
      "Training batch: 170 in epoch:1, training batch loss:0.4364\n",
      "Training batch: 175 in epoch:1, training batch loss:0.4381\n",
      "Training batch: 180 in epoch:1, training batch loss:0.4388\n",
      "Training batch: 185 in epoch:1, training batch loss:0.4445\n",
      "Training batch: 190 in epoch:1, training batch loss:0.4199\n",
      "Training batch: 195 in epoch:1, training batch loss:0.4355\n",
      "Training batch: 200 in epoch:1, training batch loss:0.4337\n",
      "Training batch: 205 in epoch:1, training batch loss:0.4468\n",
      "Training batch: 210 in epoch:1, training batch loss:0.4386\n",
      "Training batch: 215 in epoch:1, training batch loss:0.4273\n",
      "Training batch: 220 in epoch:1, training batch loss:0.4382\n",
      "Training batch: 225 in epoch:1, training batch loss:0.4382\n",
      "Training batch: 230 in epoch:1, training batch loss:0.4358\n",
      "Training batch: 235 in epoch:1, training batch loss:0.4504\n",
      "Training batch: 240 in epoch:1, training batch loss:0.4300\n",
      "Training batch: 245 in epoch:1, training batch loss:0.4408\n",
      "Training batch: 250 in epoch:1, training batch loss:0.4245\n",
      "Training batch: 255 in epoch:1, training batch loss:0.4291\n",
      "Training batch: 260 in epoch:1, training batch loss:0.4417\n",
      "Training batch: 265 in epoch:1, training batch loss:0.4393\n",
      "Training batch: 270 in epoch:1, training batch loss:0.4314\n",
      "Training batch: 275 in epoch:1, training batch loss:0.4278\n",
      "Training batch: 280 in epoch:1, training batch loss:0.4393\n",
      "Training batch: 285 in epoch:1, training batch loss:0.4492\n",
      "Training batch: 290 in epoch:1, training batch loss:0.4407\n",
      "Training batch: 295 in epoch:1, training batch loss:0.4250\n",
      "Training batch: 300 in epoch:1, training batch loss:0.4325\n",
      "Training batch: 305 in epoch:1, training batch loss:0.4375\n",
      "Training batch: 310 in epoch:1, training batch loss:0.4414\n",
      "Training batch: 315 in epoch:1, training batch loss:0.4506\n",
      "Training batch: 320 in epoch:1, training batch loss:0.4348\n",
      "Training batch: 325 in epoch:1, training batch loss:0.4423\n",
      "Training batch: 330 in epoch:1, training batch loss:0.4451\n",
      "Training batch: 335 in epoch:1, training batch loss:0.4262\n",
      "Training batch: 340 in epoch:1, training batch loss:0.4496\n",
      "Training batch: 345 in epoch:1, training batch loss:0.4404\n",
      "Training batch: 350 in epoch:1, training batch loss:0.4323\n",
      "Training batch: 355 in epoch:1, training batch loss:0.4462\n",
      "Training batch: 360 in epoch:1, training batch loss:0.4436\n",
      "Training batch: 365 in epoch:1, training batch loss:0.4320\n",
      "Training batch: 370 in epoch:1, training batch loss:0.4381\n",
      "Training batch: 375 in epoch:1, training batch loss:0.4537\n",
      "Training batch: 380 in epoch:1, training batch loss:0.4257\n",
      "Training batch: 385 in epoch:1, training batch loss:0.4257\n",
      "Training batch: 390 in epoch:1, training batch loss:0.4385\n",
      "Training batch: 395 in epoch:1, training batch loss:0.4357\n",
      "Training batch: 400 in epoch:1, training batch loss:0.4318\n",
      "Training batch: 405 in epoch:1, training batch loss:0.4324\n",
      "Training batch: 410 in epoch:1, training batch loss:0.4346\n",
      "Training batch: 415 in epoch:1, training batch loss:0.4346\n",
      "Training batch: 420 in epoch:1, training batch loss:0.4255\n",
      "Training batch: 425 in epoch:1, training batch loss:0.4405\n",
      "Training batch: 430 in epoch:1, training batch loss:0.4356\n",
      "Training batch: 435 in epoch:1, training batch loss:0.4314\n",
      "Training batch: 440 in epoch:1, training batch loss:0.4339\n",
      "Training batch: 445 in epoch:1, training batch loss:0.4277\n",
      "Training batch: 450 in epoch:1, training batch loss:0.4303\n",
      "Training batch: 455 in epoch:1, training batch loss:0.4379\n",
      "Training batch: 460 in epoch:1, training batch loss:0.4406\n",
      "Training batch: 465 in epoch:1, training batch loss:0.4253\n",
      "Training batch: 470 in epoch:1, training batch loss:0.4382\n",
      "Training batch: 475 in epoch:1, training batch loss:0.4286\n",
      "Training batch: 480 in epoch:1, training batch loss:0.4435\n",
      "Training batch: 485 in epoch:1, training batch loss:0.4300\n",
      "Training batch: 490 in epoch:1, training batch loss:0.4400\n",
      "Training batch: 495 in epoch:1, training batch loss:0.4481\n",
      "Training batch: 500 in epoch:1, training batch loss:0.4428\n",
      "Training batch: 505 in epoch:1, training batch loss:0.4332\n",
      "Training batch: 510 in epoch:1, training batch loss:0.4340\n",
      "Training batch: 515 in epoch:1, training batch loss:0.4311\n",
      "Training batch: 520 in epoch:1, training batch loss:0.4395\n",
      "Training batch: 525 in epoch:1, training batch loss:0.4389\n",
      "Training batch: 530 in epoch:1, training batch loss:0.4301\n",
      "Training batch: 535 in epoch:1, training batch loss:0.4292\n",
      "Training batch: 540 in epoch:1, training batch loss:0.4407\n",
      "Training batch: 545 in epoch:1, training batch loss:0.4298\n",
      "Training batch: 550 in epoch:1, training batch loss:0.4412\n",
      "Training batch: 555 in epoch:1, training batch loss:0.4300\n",
      "Training batch: 560 in epoch:1, training batch loss:0.4421\n",
      "Training batch: 565 in epoch:1, training batch loss:0.4370\n",
      "Training batch: 570 in epoch:1, training batch loss:0.4410\n",
      "Training batch: 575 in epoch:1, training batch loss:0.4413\n",
      "2021-04-21 15:47:38 | epoch: 0002/100, training time: 1245.3s, inference time: 300.6s\n",
      "train loss: 0.4371, val_loss: 0.4361\n",
      "val loss decrease from 0.4412 to 0.4361, saving model to ./Model/GMAN/gman4_tjwind(1).pkl\n",
      "Training batch: 5 in epoch:2, training batch loss:0.4307\n",
      "Training batch: 10 in epoch:2, training batch loss:0.4267\n",
      "Training batch: 15 in epoch:2, training batch loss:0.4324\n",
      "Training batch: 20 in epoch:2, training batch loss:0.4443\n",
      "Training batch: 25 in epoch:2, training batch loss:0.4243\n",
      "Training batch: 30 in epoch:2, training batch loss:0.4356\n",
      "Training batch: 35 in epoch:2, training batch loss:0.4430\n",
      "Training batch: 40 in epoch:2, training batch loss:0.4372\n",
      "Training batch: 45 in epoch:2, training batch loss:0.4371\n",
      "Training batch: 50 in epoch:2, training batch loss:0.4298\n",
      "Training batch: 55 in epoch:2, training batch loss:0.4390\n",
      "Training batch: 60 in epoch:2, training batch loss:0.4396\n",
      "Training batch: 65 in epoch:2, training batch loss:0.4329\n",
      "Training batch: 70 in epoch:2, training batch loss:0.4286\n",
      "Training batch: 75 in epoch:2, training batch loss:0.4521\n",
      "Training batch: 80 in epoch:2, training batch loss:0.4339\n",
      "Training batch: 85 in epoch:2, training batch loss:0.4360\n",
      "Training batch: 90 in epoch:2, training batch loss:0.4362\n",
      "Training batch: 95 in epoch:2, training batch loss:0.4360\n",
      "Training batch: 100 in epoch:2, training batch loss:0.4333\n",
      "Training batch: 105 in epoch:2, training batch loss:0.4370\n",
      "Training batch: 110 in epoch:2, training batch loss:0.4312\n",
      "Training batch: 115 in epoch:2, training batch loss:0.4253\n",
      "Training batch: 120 in epoch:2, training batch loss:0.4263\n",
      "Training batch: 125 in epoch:2, training batch loss:0.4329\n",
      "Training batch: 130 in epoch:2, training batch loss:0.4284\n",
      "Training batch: 135 in epoch:2, training batch loss:0.4416\n",
      "Training batch: 140 in epoch:2, training batch loss:0.4291\n",
      "Training batch: 145 in epoch:2, training batch loss:0.4375\n",
      "Training batch: 150 in epoch:2, training batch loss:0.4352\n",
      "Training batch: 155 in epoch:2, training batch loss:0.4345\n",
      "Training batch: 160 in epoch:2, training batch loss:0.4390\n",
      "Training batch: 165 in epoch:2, training batch loss:0.4353\n",
      "Training batch: 170 in epoch:2, training batch loss:0.4373\n",
      "Training batch: 175 in epoch:2, training batch loss:0.4328\n",
      "Training batch: 180 in epoch:2, training batch loss:0.4327\n",
      "Training batch: 185 in epoch:2, training batch loss:0.4318\n",
      "Training batch: 190 in epoch:2, training batch loss:0.4423\n",
      "Training batch: 195 in epoch:2, training batch loss:0.4316\n",
      "Training batch: 200 in epoch:2, training batch loss:0.4335\n",
      "Training batch: 205 in epoch:2, training batch loss:0.4365\n",
      "Training batch: 210 in epoch:2, training batch loss:0.4339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 215 in epoch:2, training batch loss:0.4184\n",
      "Training batch: 220 in epoch:2, training batch loss:0.4431\n",
      "Training batch: 225 in epoch:2, training batch loss:0.4395\n",
      "Training batch: 230 in epoch:2, training batch loss:0.4309\n",
      "Training batch: 235 in epoch:2, training batch loss:0.4395\n",
      "Training batch: 240 in epoch:2, training batch loss:0.4382\n",
      "Training batch: 245 in epoch:2, training batch loss:0.4354\n",
      "Training batch: 250 in epoch:2, training batch loss:0.4388\n",
      "Training batch: 255 in epoch:2, training batch loss:0.4212\n",
      "Training batch: 260 in epoch:2, training batch loss:0.4497\n",
      "Training batch: 265 in epoch:2, training batch loss:0.4272\n",
      "Training batch: 270 in epoch:2, training batch loss:0.4281\n",
      "Training batch: 275 in epoch:2, training batch loss:0.4317\n",
      "Training batch: 280 in epoch:2, training batch loss:0.4428\n",
      "Training batch: 285 in epoch:2, training batch loss:0.4237\n",
      "Training batch: 290 in epoch:2, training batch loss:0.4232\n",
      "Training batch: 295 in epoch:2, training batch loss:0.4262\n",
      "Training batch: 300 in epoch:2, training batch loss:0.4248\n",
      "Training batch: 305 in epoch:2, training batch loss:0.4282\n",
      "Training batch: 310 in epoch:2, training batch loss:0.4313\n",
      "Training batch: 315 in epoch:2, training batch loss:0.4359\n",
      "Training batch: 320 in epoch:2, training batch loss:0.4373\n",
      "Training batch: 325 in epoch:2, training batch loss:0.4300\n",
      "Training batch: 330 in epoch:2, training batch loss:0.4314\n",
      "Training batch: 335 in epoch:2, training batch loss:0.4499\n",
      "Training batch: 340 in epoch:2, training batch loss:0.4385\n",
      "Training batch: 345 in epoch:2, training batch loss:0.4341\n",
      "Training batch: 350 in epoch:2, training batch loss:0.4483\n",
      "Training batch: 355 in epoch:2, training batch loss:0.4377\n",
      "Training batch: 360 in epoch:2, training batch loss:0.4324\n",
      "Training batch: 365 in epoch:2, training batch loss:0.4362\n",
      "Training batch: 370 in epoch:2, training batch loss:0.4301\n",
      "Training batch: 375 in epoch:2, training batch loss:0.4390\n",
      "Training batch: 380 in epoch:2, training batch loss:0.4413\n",
      "Training batch: 385 in epoch:2, training batch loss:0.4275\n",
      "Training batch: 390 in epoch:2, training batch loss:0.4339\n",
      "Training batch: 395 in epoch:2, training batch loss:0.4262\n",
      "Training batch: 400 in epoch:2, training batch loss:0.4397\n",
      "Training batch: 405 in epoch:2, training batch loss:0.4424\n",
      "Training batch: 410 in epoch:2, training batch loss:0.4502\n",
      "Training batch: 415 in epoch:2, training batch loss:0.4403\n",
      "Training batch: 420 in epoch:2, training batch loss:0.4359\n",
      "Training batch: 425 in epoch:2, training batch loss:0.4302\n",
      "Training batch: 430 in epoch:2, training batch loss:0.4299\n",
      "Training batch: 435 in epoch:2, training batch loss:0.4252\n",
      "Training batch: 440 in epoch:2, training batch loss:0.4326\n",
      "Training batch: 445 in epoch:2, training batch loss:0.4327\n",
      "Training batch: 450 in epoch:2, training batch loss:0.4245\n",
      "Training batch: 455 in epoch:2, training batch loss:0.4262\n",
      "Training batch: 460 in epoch:2, training batch loss:0.4451\n",
      "Training batch: 465 in epoch:2, training batch loss:0.4354\n",
      "Training batch: 470 in epoch:2, training batch loss:0.4325\n",
      "Training batch: 475 in epoch:2, training batch loss:0.4374\n",
      "Training batch: 480 in epoch:2, training batch loss:0.4230\n",
      "Training batch: 485 in epoch:2, training batch loss:0.4266\n",
      "Training batch: 490 in epoch:2, training batch loss:0.4223\n",
      "Training batch: 495 in epoch:2, training batch loss:0.4323\n",
      "Training batch: 500 in epoch:2, training batch loss:0.4288\n",
      "Training batch: 505 in epoch:2, training batch loss:0.4299\n",
      "Training batch: 510 in epoch:2, training batch loss:0.4207\n",
      "Training batch: 515 in epoch:2, training batch loss:0.4420\n",
      "Training batch: 520 in epoch:2, training batch loss:0.4227\n",
      "Training batch: 525 in epoch:2, training batch loss:0.4290\n",
      "Training batch: 530 in epoch:2, training batch loss:0.4416\n",
      "Training batch: 535 in epoch:2, training batch loss:0.4315\n",
      "Training batch: 540 in epoch:2, training batch loss:0.4243\n",
      "Training batch: 545 in epoch:2, training batch loss:0.4295\n",
      "Training batch: 550 in epoch:2, training batch loss:0.4352\n",
      "Training batch: 555 in epoch:2, training batch loss:0.4410\n",
      "Training batch: 560 in epoch:2, training batch loss:0.4330\n",
      "Training batch: 565 in epoch:2, training batch loss:0.4381\n",
      "Training batch: 570 in epoch:2, training batch loss:0.4297\n",
      "Training batch: 575 in epoch:2, training batch loss:0.4220\n",
      "2021-04-21 16:13:32 | epoch: 0003/100, training time: 1246.5s, inference time: 307.1s\n",
      "train loss: 0.4338, val_loss: 0.4354\n",
      "val loss decrease from 0.4361 to 0.4354, saving model to ./Model/GMAN/gman4_tjwind(1).pkl\n",
      "Training batch: 5 in epoch:3, training batch loss:0.4299\n",
      "Training batch: 10 in epoch:3, training batch loss:0.4344\n",
      "Training batch: 15 in epoch:3, training batch loss:0.4259\n",
      "Training batch: 20 in epoch:3, training batch loss:0.4332\n",
      "Training batch: 25 in epoch:3, training batch loss:0.4288\n",
      "Training batch: 30 in epoch:3, training batch loss:0.4333\n",
      "Training batch: 35 in epoch:3, training batch loss:0.4385\n",
      "Training batch: 40 in epoch:3, training batch loss:0.4238\n",
      "Training batch: 45 in epoch:3, training batch loss:0.4302\n",
      "Training batch: 50 in epoch:3, training batch loss:0.4325\n",
      "Training batch: 55 in epoch:3, training batch loss:0.4285\n",
      "Training batch: 60 in epoch:3, training batch loss:0.4329\n",
      "Training batch: 65 in epoch:3, training batch loss:0.4391\n",
      "Training batch: 70 in epoch:3, training batch loss:0.4246\n",
      "Training batch: 75 in epoch:3, training batch loss:0.4356\n",
      "Training batch: 80 in epoch:3, training batch loss:0.4366\n",
      "Training batch: 85 in epoch:3, training batch loss:0.4422\n",
      "Training batch: 90 in epoch:3, training batch loss:0.4304\n",
      "Training batch: 95 in epoch:3, training batch loss:0.4405\n",
      "Training batch: 100 in epoch:3, training batch loss:0.4292\n",
      "Training batch: 105 in epoch:3, training batch loss:0.4299\n",
      "Training batch: 110 in epoch:3, training batch loss:0.4275\n",
      "Training batch: 115 in epoch:3, training batch loss:0.4440\n",
      "Training batch: 120 in epoch:3, training batch loss:0.4356\n",
      "Training batch: 125 in epoch:3, training batch loss:0.4307\n",
      "Training batch: 130 in epoch:3, training batch loss:0.4363\n",
      "Training batch: 135 in epoch:3, training batch loss:0.4330\n",
      "Training batch: 140 in epoch:3, training batch loss:0.4226\n",
      "Training batch: 145 in epoch:3, training batch loss:0.4333\n",
      "Training batch: 150 in epoch:3, training batch loss:0.4257\n",
      "Training batch: 155 in epoch:3, training batch loss:0.4307\n",
      "Training batch: 160 in epoch:3, training batch loss:0.4390\n",
      "Training batch: 165 in epoch:3, training batch loss:0.4275\n",
      "Training batch: 170 in epoch:3, training batch loss:0.4375\n",
      "Training batch: 175 in epoch:3, training batch loss:0.4275\n",
      "Training batch: 180 in epoch:3, training batch loss:0.4374\n",
      "Training batch: 185 in epoch:3, training batch loss:0.4332\n",
      "Training batch: 190 in epoch:3, training batch loss:0.4248\n",
      "Training batch: 195 in epoch:3, training batch loss:0.4281\n",
      "Training batch: 200 in epoch:3, training batch loss:0.4331\n",
      "Training batch: 205 in epoch:3, training batch loss:0.4333\n",
      "Training batch: 210 in epoch:3, training batch loss:0.4461\n",
      "Training batch: 215 in epoch:3, training batch loss:0.4312\n",
      "Training batch: 220 in epoch:3, training batch loss:0.4331\n",
      "Training batch: 225 in epoch:3, training batch loss:0.4327\n",
      "Training batch: 230 in epoch:3, training batch loss:0.4300\n",
      "Training batch: 235 in epoch:3, training batch loss:0.4183\n",
      "Training batch: 240 in epoch:3, training batch loss:0.4370\n",
      "Training batch: 245 in epoch:3, training batch loss:0.4305\n",
      "Training batch: 250 in epoch:3, training batch loss:0.4379\n",
      "Training batch: 255 in epoch:3, training batch loss:0.4387\n",
      "Training batch: 260 in epoch:3, training batch loss:0.4317\n",
      "Training batch: 265 in epoch:3, training batch loss:0.4201\n",
      "Training batch: 270 in epoch:3, training batch loss:0.4465\n",
      "Training batch: 275 in epoch:3, training batch loss:0.4272\n",
      "Training batch: 280 in epoch:3, training batch loss:0.4318\n",
      "Training batch: 285 in epoch:3, training batch loss:0.4412\n",
      "Training batch: 290 in epoch:3, training batch loss:0.4239\n",
      "Training batch: 295 in epoch:3, training batch loss:0.4325\n",
      "Training batch: 300 in epoch:3, training batch loss:0.4360\n",
      "Training batch: 305 in epoch:3, training batch loss:0.4305\n",
      "Training batch: 310 in epoch:3, training batch loss:0.4363\n",
      "Training batch: 315 in epoch:3, training batch loss:0.4515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 320 in epoch:3, training batch loss:0.4343\n",
      "Training batch: 325 in epoch:3, training batch loss:0.4347\n",
      "Training batch: 330 in epoch:3, training batch loss:0.4285\n",
      "Training batch: 335 in epoch:3, training batch loss:0.4278\n",
      "Training batch: 340 in epoch:3, training batch loss:0.4331\n",
      "Training batch: 345 in epoch:3, training batch loss:0.4344\n",
      "Training batch: 350 in epoch:3, training batch loss:0.4334\n",
      "Training batch: 355 in epoch:3, training batch loss:0.4229\n",
      "Training batch: 360 in epoch:3, training batch loss:0.4344\n",
      "Training batch: 365 in epoch:3, training batch loss:0.4396\n",
      "Training batch: 370 in epoch:3, training batch loss:0.4295\n",
      "Training batch: 375 in epoch:3, training batch loss:0.4403\n",
      "Training batch: 380 in epoch:3, training batch loss:0.4290\n",
      "Training batch: 385 in epoch:3, training batch loss:0.4409\n",
      "Training batch: 390 in epoch:3, training batch loss:0.4296\n",
      "Training batch: 395 in epoch:3, training batch loss:0.4328\n",
      "Training batch: 400 in epoch:3, training batch loss:0.4312\n",
      "Training batch: 405 in epoch:3, training batch loss:0.4267\n",
      "Training batch: 410 in epoch:3, training batch loss:0.4259\n",
      "Training batch: 415 in epoch:3, training batch loss:0.4328\n",
      "Training batch: 420 in epoch:3, training batch loss:0.4387\n",
      "Training batch: 425 in epoch:3, training batch loss:0.4316\n",
      "Training batch: 430 in epoch:3, training batch loss:0.4354\n",
      "Training batch: 435 in epoch:3, training batch loss:0.4316\n",
      "Training batch: 440 in epoch:3, training batch loss:0.4227\n",
      "Training batch: 445 in epoch:3, training batch loss:0.4347\n",
      "Training batch: 450 in epoch:3, training batch loss:0.4388\n",
      "Training batch: 455 in epoch:3, training batch loss:0.4335\n",
      "Training batch: 460 in epoch:3, training batch loss:0.4365\n",
      "Training batch: 465 in epoch:3, training batch loss:0.4322\n",
      "Training batch: 470 in epoch:3, training batch loss:0.4220\n",
      "Training batch: 475 in epoch:3, training batch loss:0.4289\n",
      "Training batch: 480 in epoch:3, training batch loss:0.4323\n",
      "Training batch: 485 in epoch:3, training batch loss:0.4272\n",
      "Training batch: 490 in epoch:3, training batch loss:0.4349\n",
      "Training batch: 495 in epoch:3, training batch loss:0.4353\n",
      "Training batch: 500 in epoch:3, training batch loss:0.4291\n",
      "Training batch: 505 in epoch:3, training batch loss:0.4264\n",
      "Training batch: 510 in epoch:3, training batch loss:0.4342\n",
      "Training batch: 515 in epoch:3, training batch loss:0.4421\n",
      "Training batch: 520 in epoch:3, training batch loss:0.4308\n",
      "Training batch: 525 in epoch:3, training batch loss:0.4377\n",
      "Training batch: 530 in epoch:3, training batch loss:0.4350\n",
      "Training batch: 535 in epoch:3, training batch loss:0.4236\n",
      "Training batch: 540 in epoch:3, training batch loss:0.4212\n",
      "Training batch: 545 in epoch:3, training batch loss:0.4484\n",
      "Training batch: 550 in epoch:3, training batch loss:0.4373\n",
      "Training batch: 555 in epoch:3, training batch loss:0.4329\n",
      "Training batch: 560 in epoch:3, training batch loss:0.4382\n",
      "Training batch: 565 in epoch:3, training batch loss:0.4268\n",
      "Training batch: 570 in epoch:3, training batch loss:0.4300\n",
      "Training batch: 575 in epoch:3, training batch loss:0.4326\n",
      "2021-04-21 16:39:12 | epoch: 0004/100, training time: 1237.1s, inference time: 302.8s\n",
      "train loss: 0.4321, val_loss: 0.4338\n",
      "val loss decrease from 0.4354 to 0.4338, saving model to ./Model/GMAN/gman4_tjwind(1).pkl\n",
      "Training batch: 5 in epoch:4, training batch loss:0.4184\n",
      "Training batch: 10 in epoch:4, training batch loss:0.4361\n",
      "Training batch: 15 in epoch:4, training batch loss:0.4275\n",
      "Training batch: 20 in epoch:4, training batch loss:0.4305\n",
      "Training batch: 25 in epoch:4, training batch loss:0.4356\n",
      "Training batch: 30 in epoch:4, training batch loss:0.4340\n",
      "Training batch: 35 in epoch:4, training batch loss:0.4282\n",
      "Training batch: 40 in epoch:4, training batch loss:0.4317\n",
      "Training batch: 45 in epoch:4, training batch loss:0.4301\n",
      "Training batch: 50 in epoch:4, training batch loss:0.4434\n",
      "Training batch: 55 in epoch:4, training batch loss:0.4289\n",
      "Training batch: 60 in epoch:4, training batch loss:0.4204\n",
      "Training batch: 65 in epoch:4, training batch loss:0.4296\n",
      "Training batch: 70 in epoch:4, training batch loss:0.4395\n",
      "Training batch: 75 in epoch:4, training batch loss:0.4385\n",
      "Training batch: 80 in epoch:4, training batch loss:0.4268\n",
      "Training batch: 85 in epoch:4, training batch loss:0.4240\n",
      "Training batch: 90 in epoch:4, training batch loss:0.4367\n",
      "Training batch: 95 in epoch:4, training batch loss:0.4258\n",
      "Training batch: 100 in epoch:4, training batch loss:0.4381\n",
      "Training batch: 105 in epoch:4, training batch loss:0.4350\n",
      "Training batch: 110 in epoch:4, training batch loss:0.4318\n",
      "Training batch: 115 in epoch:4, training batch loss:0.4205\n",
      "Training batch: 120 in epoch:4, training batch loss:0.4355\n",
      "Training batch: 125 in epoch:4, training batch loss:0.4302\n",
      "Training batch: 130 in epoch:4, training batch loss:0.4393\n",
      "Training batch: 135 in epoch:4, training batch loss:0.4225\n",
      "Training batch: 140 in epoch:4, training batch loss:0.4304\n",
      "Training batch: 145 in epoch:4, training batch loss:0.4301\n",
      "Training batch: 150 in epoch:4, training batch loss:0.4205\n",
      "Training batch: 155 in epoch:4, training batch loss:0.4142\n",
      "Training batch: 160 in epoch:4, training batch loss:0.4300\n",
      "Training batch: 165 in epoch:4, training batch loss:0.4342\n",
      "Training batch: 170 in epoch:4, training batch loss:0.4206\n",
      "Training batch: 175 in epoch:4, training batch loss:0.4398\n",
      "Training batch: 180 in epoch:4, training batch loss:0.4286\n",
      "Training batch: 185 in epoch:4, training batch loss:0.4271\n",
      "Training batch: 190 in epoch:4, training batch loss:0.4287\n",
      "Training batch: 195 in epoch:4, training batch loss:0.4253\n",
      "Training batch: 200 in epoch:4, training batch loss:0.4278\n",
      "Training batch: 205 in epoch:4, training batch loss:0.4203\n",
      "Training batch: 210 in epoch:4, training batch loss:0.4309\n",
      "Training batch: 215 in epoch:4, training batch loss:0.4287\n",
      "Training batch: 220 in epoch:4, training batch loss:0.4312\n",
      "Training batch: 225 in epoch:4, training batch loss:0.4244\n",
      "Training batch: 230 in epoch:4, training batch loss:0.4338\n",
      "Training batch: 235 in epoch:4, training batch loss:0.4296\n",
      "Training batch: 240 in epoch:4, training batch loss:0.4342\n",
      "Training batch: 245 in epoch:4, training batch loss:0.4266\n",
      "Training batch: 250 in epoch:4, training batch loss:0.4378\n",
      "Training batch: 255 in epoch:4, training batch loss:0.4403\n",
      "Training batch: 260 in epoch:4, training batch loss:0.4315\n",
      "Training batch: 265 in epoch:4, training batch loss:0.4330\n",
      "Training batch: 270 in epoch:4, training batch loss:0.4301\n",
      "Training batch: 275 in epoch:4, training batch loss:0.4326\n",
      "Training batch: 280 in epoch:4, training batch loss:0.4335\n",
      "Training batch: 285 in epoch:4, training batch loss:0.4271\n",
      "Training batch: 290 in epoch:4, training batch loss:0.4393\n",
      "Training batch: 295 in epoch:4, training batch loss:0.4296\n",
      "Training batch: 300 in epoch:4, training batch loss:0.4296\n",
      "Training batch: 305 in epoch:4, training batch loss:0.4399\n",
      "Training batch: 310 in epoch:4, training batch loss:0.4333\n",
      "Training batch: 315 in epoch:4, training batch loss:0.4206\n",
      "Training batch: 320 in epoch:4, training batch loss:0.4224\n",
      "Training batch: 325 in epoch:4, training batch loss:0.4276\n",
      "Training batch: 330 in epoch:4, training batch loss:0.4369\n",
      "Training batch: 335 in epoch:4, training batch loss:0.4404\n",
      "Training batch: 340 in epoch:4, training batch loss:0.4239\n",
      "Training batch: 345 in epoch:4, training batch loss:0.4418\n",
      "Training batch: 350 in epoch:4, training batch loss:0.4278\n",
      "Training batch: 355 in epoch:4, training batch loss:0.4397\n",
      "Training batch: 360 in epoch:4, training batch loss:0.4220\n",
      "Training batch: 365 in epoch:4, training batch loss:0.4274\n",
      "Training batch: 370 in epoch:4, training batch loss:0.4225\n",
      "Training batch: 375 in epoch:4, training batch loss:0.4268\n",
      "Training batch: 380 in epoch:4, training batch loss:0.4347\n",
      "Training batch: 385 in epoch:4, training batch loss:0.4476\n",
      "Training batch: 390 in epoch:4, training batch loss:0.4236\n",
      "Training batch: 395 in epoch:4, training batch loss:0.4421\n",
      "Training batch: 400 in epoch:4, training batch loss:0.4396\n",
      "Training batch: 405 in epoch:4, training batch loss:0.4291\n",
      "Training batch: 410 in epoch:4, training batch loss:0.4302\n",
      "Training batch: 415 in epoch:4, training batch loss:0.4303\n",
      "Training batch: 420 in epoch:4, training batch loss:0.4358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 425 in epoch:4, training batch loss:0.4376\n",
      "Training batch: 430 in epoch:4, training batch loss:0.4265\n",
      "Training batch: 435 in epoch:4, training batch loss:0.4427\n",
      "Training batch: 440 in epoch:4, training batch loss:0.4260\n",
      "Training batch: 445 in epoch:4, training batch loss:0.4327\n",
      "Training batch: 450 in epoch:4, training batch loss:0.4273\n",
      "Training batch: 455 in epoch:4, training batch loss:0.4248\n",
      "Training batch: 460 in epoch:4, training batch loss:0.4345\n",
      "Training batch: 465 in epoch:4, training batch loss:0.4232\n",
      "Training batch: 470 in epoch:4, training batch loss:0.4283\n",
      "Training batch: 475 in epoch:4, training batch loss:0.4311\n",
      "Training batch: 480 in epoch:4, training batch loss:0.4353\n",
      "Training batch: 485 in epoch:4, training batch loss:0.4326\n",
      "Training batch: 490 in epoch:4, training batch loss:0.4327\n",
      "Training batch: 495 in epoch:4, training batch loss:0.4315\n",
      "Training batch: 500 in epoch:4, training batch loss:0.4288\n",
      "Training batch: 505 in epoch:4, training batch loss:0.4265\n",
      "Training batch: 510 in epoch:4, training batch loss:0.4321\n",
      "Training batch: 515 in epoch:4, training batch loss:0.4224\n",
      "Training batch: 520 in epoch:4, training batch loss:0.4323\n",
      "Training batch: 525 in epoch:4, training batch loss:0.4385\n",
      "Training batch: 530 in epoch:4, training batch loss:0.4189\n",
      "Training batch: 535 in epoch:4, training batch loss:0.4318\n",
      "Training batch: 540 in epoch:4, training batch loss:0.4326\n",
      "Training batch: 545 in epoch:4, training batch loss:0.4252\n",
      "Training batch: 550 in epoch:4, training batch loss:0.4352\n",
      "Training batch: 555 in epoch:4, training batch loss:0.4226\n",
      "Training batch: 560 in epoch:4, training batch loss:0.4342\n",
      "Training batch: 565 in epoch:4, training batch loss:0.4292\n",
      "Training batch: 570 in epoch:4, training batch loss:0.4292\n",
      "Training batch: 575 in epoch:4, training batch loss:0.4194\n",
      "2021-04-21 17:04:46 | epoch: 0005/100, training time: 1230.9s, inference time: 302.9s\n",
      "train loss: 0.4309, val_loss: 0.4342\n",
      "Training batch: 5 in epoch:5, training batch loss:0.4216\n",
      "Training batch: 10 in epoch:5, training batch loss:0.4307\n",
      "Training batch: 15 in epoch:5, training batch loss:0.4301\n",
      "Training batch: 20 in epoch:5, training batch loss:0.4282\n",
      "Training batch: 25 in epoch:5, training batch loss:0.4378\n",
      "Training batch: 30 in epoch:5, training batch loss:0.4320\n",
      "Training batch: 35 in epoch:5, training batch loss:0.4388\n",
      "Training batch: 40 in epoch:5, training batch loss:0.4295\n",
      "Training batch: 45 in epoch:5, training batch loss:0.4419\n",
      "Training batch: 50 in epoch:5, training batch loss:0.4171\n",
      "Training batch: 55 in epoch:5, training batch loss:0.4359\n",
      "Training batch: 60 in epoch:5, training batch loss:0.4348\n",
      "Training batch: 65 in epoch:5, training batch loss:0.4282\n",
      "Training batch: 70 in epoch:5, training batch loss:0.4298\n",
      "Training batch: 75 in epoch:5, training batch loss:0.4377\n",
      "Training batch: 80 in epoch:5, training batch loss:0.4222\n",
      "Training batch: 85 in epoch:5, training batch loss:0.4379\n",
      "Training batch: 90 in epoch:5, training batch loss:0.4273\n",
      "Training batch: 95 in epoch:5, training batch loss:0.4317\n",
      "Training batch: 100 in epoch:5, training batch loss:0.4285\n",
      "Training batch: 105 in epoch:5, training batch loss:0.4350\n",
      "Training batch: 110 in epoch:5, training batch loss:0.4333\n",
      "Training batch: 115 in epoch:5, training batch loss:0.4232\n",
      "Training batch: 120 in epoch:5, training batch loss:0.4210\n",
      "Training batch: 125 in epoch:5, training batch loss:0.4246\n",
      "Training batch: 130 in epoch:5, training batch loss:0.4190\n",
      "Training batch: 135 in epoch:5, training batch loss:0.4386\n",
      "Training batch: 140 in epoch:5, training batch loss:0.4444\n",
      "Training batch: 145 in epoch:5, training batch loss:0.4305\n",
      "Training batch: 150 in epoch:5, training batch loss:0.4286\n",
      "Training batch: 155 in epoch:5, training batch loss:0.4458\n",
      "Training batch: 160 in epoch:5, training batch loss:0.4307\n",
      "Training batch: 165 in epoch:5, training batch loss:0.4356\n",
      "Training batch: 170 in epoch:5, training batch loss:0.4258\n",
      "Training batch: 175 in epoch:5, training batch loss:0.4329\n",
      "Training batch: 180 in epoch:5, training batch loss:0.4492\n",
      "Training batch: 185 in epoch:5, training batch loss:0.4382\n",
      "Training batch: 190 in epoch:5, training batch loss:0.4334\n",
      "Training batch: 195 in epoch:5, training batch loss:0.4256\n",
      "Training batch: 200 in epoch:5, training batch loss:0.4303\n",
      "Training batch: 205 in epoch:5, training batch loss:0.4273\n",
      "Training batch: 210 in epoch:5, training batch loss:0.4240\n",
      "Training batch: 215 in epoch:5, training batch loss:0.4345\n",
      "Training batch: 220 in epoch:5, training batch loss:0.4296\n",
      "Training batch: 225 in epoch:5, training batch loss:0.4352\n",
      "Training batch: 230 in epoch:5, training batch loss:0.4321\n",
      "Training batch: 235 in epoch:5, training batch loss:0.4306\n",
      "Training batch: 240 in epoch:5, training batch loss:0.4229\n",
      "Training batch: 245 in epoch:5, training batch loss:0.4399\n",
      "Training batch: 250 in epoch:5, training batch loss:0.4282\n",
      "Training batch: 255 in epoch:5, training batch loss:0.4266\n",
      "Training batch: 260 in epoch:5, training batch loss:0.4320\n",
      "Training batch: 265 in epoch:5, training batch loss:0.4258\n",
      "Training batch: 270 in epoch:5, training batch loss:0.4224\n",
      "Training batch: 275 in epoch:5, training batch loss:0.4311\n",
      "Training batch: 280 in epoch:5, training batch loss:0.4323\n",
      "Training batch: 285 in epoch:5, training batch loss:0.4320\n",
      "Training batch: 290 in epoch:5, training batch loss:0.4264\n",
      "Training batch: 295 in epoch:5, training batch loss:0.4327\n",
      "Training batch: 300 in epoch:5, training batch loss:0.4242\n",
      "Training batch: 305 in epoch:5, training batch loss:0.4278\n",
      "Training batch: 310 in epoch:5, training batch loss:0.4258\n",
      "Training batch: 315 in epoch:5, training batch loss:0.4408\n",
      "Training batch: 320 in epoch:5, training batch loss:0.4332\n",
      "Training batch: 325 in epoch:5, training batch loss:0.4254\n",
      "Training batch: 330 in epoch:5, training batch loss:0.4317\n",
      "Training batch: 335 in epoch:5, training batch loss:0.4319\n",
      "Training batch: 340 in epoch:5, training batch loss:0.4281\n",
      "Training batch: 345 in epoch:5, training batch loss:0.4261\n",
      "Training batch: 350 in epoch:5, training batch loss:0.4228\n",
      "Training batch: 355 in epoch:5, training batch loss:0.4280\n",
      "Training batch: 360 in epoch:5, training batch loss:0.4299\n",
      "Training batch: 365 in epoch:5, training batch loss:0.4268\n",
      "Training batch: 370 in epoch:5, training batch loss:0.4486\n",
      "Training batch: 375 in epoch:5, training batch loss:0.4319\n",
      "Training batch: 380 in epoch:5, training batch loss:0.4298\n",
      "Training batch: 385 in epoch:5, training batch loss:0.4253\n",
      "Training batch: 390 in epoch:5, training batch loss:0.4314\n",
      "Training batch: 395 in epoch:5, training batch loss:0.4243\n",
      "Training batch: 400 in epoch:5, training batch loss:0.4443\n",
      "Training batch: 405 in epoch:5, training batch loss:0.4160\n",
      "Training batch: 410 in epoch:5, training batch loss:0.4299\n",
      "Training batch: 415 in epoch:5, training batch loss:0.4348\n",
      "Training batch: 420 in epoch:5, training batch loss:0.4316\n",
      "Training batch: 425 in epoch:5, training batch loss:0.4257\n",
      "Training batch: 430 in epoch:5, training batch loss:0.4305\n",
      "Training batch: 435 in epoch:5, training batch loss:0.4358\n",
      "Training batch: 440 in epoch:5, training batch loss:0.4212\n",
      "Training batch: 445 in epoch:5, training batch loss:0.4405\n",
      "Training batch: 450 in epoch:5, training batch loss:0.4346\n",
      "Training batch: 455 in epoch:5, training batch loss:0.4362\n",
      "Training batch: 460 in epoch:5, training batch loss:0.4177\n",
      "Training batch: 465 in epoch:5, training batch loss:0.4230\n",
      "Training batch: 470 in epoch:5, training batch loss:0.4258\n",
      "Training batch: 475 in epoch:5, training batch loss:0.4411\n",
      "Training batch: 480 in epoch:5, training batch loss:0.4395\n",
      "Training batch: 485 in epoch:5, training batch loss:0.4201\n",
      "Training batch: 490 in epoch:5, training batch loss:0.4208\n",
      "Training batch: 495 in epoch:5, training batch loss:0.4279\n",
      "Training batch: 500 in epoch:5, training batch loss:0.4271\n",
      "Training batch: 505 in epoch:5, training batch loss:0.4338\n",
      "Training batch: 510 in epoch:5, training batch loss:0.4349\n",
      "Training batch: 515 in epoch:5, training batch loss:0.4260\n",
      "Training batch: 520 in epoch:5, training batch loss:0.4341\n",
      "Training batch: 525 in epoch:5, training batch loss:0.4206\n",
      "Training batch: 530 in epoch:5, training batch loss:0.4298\n",
      "Training batch: 535 in epoch:5, training batch loss:0.4372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 540 in epoch:5, training batch loss:0.4283\n",
      "Training batch: 545 in epoch:5, training batch loss:0.4273\n",
      "Training batch: 550 in epoch:5, training batch loss:0.4222\n",
      "Training batch: 555 in epoch:5, training batch loss:0.4243\n",
      "Training batch: 560 in epoch:5, training batch loss:0.4222\n",
      "Training batch: 565 in epoch:5, training batch loss:0.4393\n",
      "Training batch: 570 in epoch:5, training batch loss:0.4334\n",
      "Training batch: 575 in epoch:5, training batch loss:0.4404\n",
      "2021-04-21 17:30:16 | epoch: 0006/100, training time: 1234.0s, inference time: 296.3s\n",
      "train loss: 0.4302, val_loss: 0.4345\n",
      "Training batch: 5 in epoch:6, training batch loss:0.4319\n",
      "Training batch: 10 in epoch:6, training batch loss:0.4238\n",
      "Training batch: 15 in epoch:6, training batch loss:0.4261\n",
      "Training batch: 20 in epoch:6, training batch loss:0.4313\n",
      "Training batch: 25 in epoch:6, training batch loss:0.4333\n",
      "Training batch: 30 in epoch:6, training batch loss:0.4358\n",
      "Training batch: 35 in epoch:6, training batch loss:0.4219\n",
      "Training batch: 40 in epoch:6, training batch loss:0.4255\n",
      "Training batch: 45 in epoch:6, training batch loss:0.4400\n",
      "Training batch: 50 in epoch:6, training batch loss:0.4331\n",
      "Training batch: 55 in epoch:6, training batch loss:0.4304\n",
      "Training batch: 60 in epoch:6, training batch loss:0.4338\n",
      "Training batch: 65 in epoch:6, training batch loss:0.4258\n",
      "Training batch: 70 in epoch:6, training batch loss:0.4191\n",
      "Training batch: 75 in epoch:6, training batch loss:0.4276\n",
      "Training batch: 80 in epoch:6, training batch loss:0.4294\n",
      "Training batch: 85 in epoch:6, training batch loss:0.4227\n",
      "Training batch: 90 in epoch:6, training batch loss:0.4378\n",
      "Training batch: 95 in epoch:6, training batch loss:0.4409\n",
      "Training batch: 100 in epoch:6, training batch loss:0.4384\n",
      "Training batch: 105 in epoch:6, training batch loss:0.4449\n",
      "Training batch: 110 in epoch:6, training batch loss:0.4259\n",
      "Training batch: 115 in epoch:6, training batch loss:0.4359\n",
      "Training batch: 120 in epoch:6, training batch loss:0.4161\n",
      "Training batch: 125 in epoch:6, training batch loss:0.4267\n",
      "Training batch: 130 in epoch:6, training batch loss:0.4342\n",
      "Training batch: 135 in epoch:6, training batch loss:0.4227\n",
      "Training batch: 140 in epoch:6, training batch loss:0.4265\n",
      "Training batch: 145 in epoch:6, training batch loss:0.4218\n",
      "Training batch: 150 in epoch:6, training batch loss:0.4281\n",
      "Training batch: 155 in epoch:6, training batch loss:0.4287\n",
      "Training batch: 160 in epoch:6, training batch loss:0.4230\n",
      "Training batch: 165 in epoch:6, training batch loss:0.4249\n",
      "Training batch: 170 in epoch:6, training batch loss:0.4288\n",
      "Training batch: 175 in epoch:6, training batch loss:0.4323\n",
      "Training batch: 180 in epoch:6, training batch loss:0.4320\n",
      "Training batch: 185 in epoch:6, training batch loss:0.4328\n",
      "Training batch: 190 in epoch:6, training batch loss:0.4222\n",
      "Training batch: 195 in epoch:6, training batch loss:0.4338\n",
      "Training batch: 200 in epoch:6, training batch loss:0.4301\n",
      "Training batch: 205 in epoch:6, training batch loss:0.4411\n",
      "Training batch: 210 in epoch:6, training batch loss:0.4279\n",
      "Training batch: 215 in epoch:6, training batch loss:0.4330\n",
      "Training batch: 220 in epoch:6, training batch loss:0.4228\n",
      "Training batch: 225 in epoch:6, training batch loss:0.4214\n",
      "Training batch: 230 in epoch:6, training batch loss:0.4301\n",
      "Training batch: 235 in epoch:6, training batch loss:0.4493\n",
      "Training batch: 240 in epoch:6, training batch loss:0.4213\n",
      "Training batch: 245 in epoch:6, training batch loss:0.4270\n",
      "Training batch: 250 in epoch:6, training batch loss:0.4359\n",
      "Training batch: 255 in epoch:6, training batch loss:0.4195\n",
      "Training batch: 260 in epoch:6, training batch loss:0.4362\n",
      "Training batch: 265 in epoch:6, training batch loss:0.4402\n",
      "Training batch: 270 in epoch:6, training batch loss:0.4285\n",
      "Training batch: 275 in epoch:6, training batch loss:0.4203\n",
      "Training batch: 280 in epoch:6, training batch loss:0.4247\n",
      "Training batch: 285 in epoch:6, training batch loss:0.4307\n",
      "Training batch: 290 in epoch:6, training batch loss:0.4288\n",
      "Training batch: 295 in epoch:6, training batch loss:0.4423\n",
      "Training batch: 300 in epoch:6, training batch loss:0.4338\n",
      "Training batch: 305 in epoch:6, training batch loss:0.4340\n",
      "Training batch: 310 in epoch:6, training batch loss:0.4366\n",
      "Training batch: 315 in epoch:6, training batch loss:0.4335\n",
      "Training batch: 320 in epoch:6, training batch loss:0.4253\n",
      "Training batch: 325 in epoch:6, training batch loss:0.4266\n",
      "Training batch: 330 in epoch:6, training batch loss:0.4249\n",
      "Training batch: 335 in epoch:6, training batch loss:0.4248\n",
      "Training batch: 340 in epoch:6, training batch loss:0.4227\n",
      "Training batch: 345 in epoch:6, training batch loss:0.4409\n",
      "Training batch: 350 in epoch:6, training batch loss:0.4357\n",
      "Training batch: 355 in epoch:6, training batch loss:0.4303\n",
      "Training batch: 360 in epoch:6, training batch loss:0.4282\n",
      "Training batch: 365 in epoch:6, training batch loss:0.4236\n",
      "Training batch: 370 in epoch:6, training batch loss:0.4208\n",
      "Training batch: 375 in epoch:6, training batch loss:0.4224\n",
      "Training batch: 380 in epoch:6, training batch loss:0.4337\n",
      "Training batch: 385 in epoch:6, training batch loss:0.4384\n",
      "Training batch: 390 in epoch:6, training batch loss:0.4254\n",
      "Training batch: 395 in epoch:6, training batch loss:0.4211\n",
      "Training batch: 400 in epoch:6, training batch loss:0.4180\n",
      "Training batch: 405 in epoch:6, training batch loss:0.4174\n",
      "Training batch: 410 in epoch:6, training batch loss:0.4256\n",
      "Training batch: 415 in epoch:6, training batch loss:0.4266\n",
      "Training batch: 420 in epoch:6, training batch loss:0.4212\n",
      "Training batch: 425 in epoch:6, training batch loss:0.4257\n",
      "Training batch: 430 in epoch:6, training batch loss:0.4391\n",
      "Training batch: 435 in epoch:6, training batch loss:0.4358\n",
      "Training batch: 440 in epoch:6, training batch loss:0.4256\n",
      "Training batch: 445 in epoch:6, training batch loss:0.4251\n",
      "Training batch: 450 in epoch:6, training batch loss:0.4278\n",
      "Training batch: 455 in epoch:6, training batch loss:0.4242\n",
      "Training batch: 460 in epoch:6, training batch loss:0.4353\n",
      "Training batch: 465 in epoch:6, training batch loss:0.4274\n",
      "Training batch: 470 in epoch:6, training batch loss:0.4235\n",
      "Training batch: 475 in epoch:6, training batch loss:0.4317\n",
      "Training batch: 480 in epoch:6, training batch loss:0.4243\n",
      "Training batch: 485 in epoch:6, training batch loss:0.4321\n",
      "Training batch: 490 in epoch:6, training batch loss:0.4293\n",
      "Training batch: 495 in epoch:6, training batch loss:0.4302\n",
      "Training batch: 500 in epoch:6, training batch loss:0.4256\n",
      "Training batch: 505 in epoch:6, training batch loss:0.4186\n",
      "Training batch: 510 in epoch:6, training batch loss:0.4341\n",
      "Training batch: 515 in epoch:6, training batch loss:0.4260\n",
      "Training batch: 520 in epoch:6, training batch loss:0.4248\n",
      "Training batch: 525 in epoch:6, training batch loss:0.4343\n",
      "Training batch: 530 in epoch:6, training batch loss:0.4159\n",
      "Training batch: 535 in epoch:6, training batch loss:0.4267\n",
      "Training batch: 540 in epoch:6, training batch loss:0.4214\n",
      "Training batch: 545 in epoch:6, training batch loss:0.4333\n",
      "Training batch: 550 in epoch:6, training batch loss:0.4284\n",
      "Training batch: 555 in epoch:6, training batch loss:0.4321\n",
      "Training batch: 560 in epoch:6, training batch loss:0.4396\n",
      "Training batch: 565 in epoch:6, training batch loss:0.4264\n",
      "Training batch: 570 in epoch:6, training batch loss:0.4342\n",
      "Training batch: 575 in epoch:6, training batch loss:0.4274\n",
      "2021-04-21 17:55:38 | epoch: 0007/100, training time: 1225.5s, inference time: 296.6s\n",
      "train loss: 0.4294, val_loss: 0.4366\n",
      "Training batch: 5 in epoch:7, training batch loss:0.4356\n",
      "Training batch: 10 in epoch:7, training batch loss:0.4351\n",
      "Training batch: 15 in epoch:7, training batch loss:0.4285\n",
      "Training batch: 20 in epoch:7, training batch loss:0.4191\n",
      "Training batch: 25 in epoch:7, training batch loss:0.4358\n",
      "Training batch: 30 in epoch:7, training batch loss:0.4380\n",
      "Training batch: 35 in epoch:7, training batch loss:0.4288\n",
      "Training batch: 40 in epoch:7, training batch loss:0.4310\n",
      "Training batch: 45 in epoch:7, training batch loss:0.4244\n",
      "Training batch: 50 in epoch:7, training batch loss:0.4365\n",
      "Training batch: 55 in epoch:7, training batch loss:0.4213\n",
      "Training batch: 60 in epoch:7, training batch loss:0.4275\n",
      "Training batch: 65 in epoch:7, training batch loss:0.4406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 70 in epoch:7, training batch loss:0.4222\n",
      "Training batch: 75 in epoch:7, training batch loss:0.4360\n",
      "Training batch: 80 in epoch:7, training batch loss:0.4261\n",
      "Training batch: 85 in epoch:7, training batch loss:0.4232\n",
      "Training batch: 90 in epoch:7, training batch loss:0.4476\n",
      "Training batch: 95 in epoch:7, training batch loss:0.4300\n",
      "Training batch: 100 in epoch:7, training batch loss:0.4294\n",
      "Training batch: 105 in epoch:7, training batch loss:0.4273\n",
      "Training batch: 110 in epoch:7, training batch loss:0.4274\n",
      "Training batch: 115 in epoch:7, training batch loss:0.4396\n",
      "Training batch: 120 in epoch:7, training batch loss:0.4245\n",
      "Training batch: 125 in epoch:7, training batch loss:0.4327\n",
      "Training batch: 130 in epoch:7, training batch loss:0.4226\n",
      "Training batch: 135 in epoch:7, training batch loss:0.4367\n",
      "Training batch: 140 in epoch:7, training batch loss:0.4359\n",
      "Training batch: 145 in epoch:7, training batch loss:0.4274\n",
      "Training batch: 150 in epoch:7, training batch loss:0.4345\n",
      "Training batch: 155 in epoch:7, training batch loss:0.4198\n",
      "Training batch: 160 in epoch:7, training batch loss:0.4186\n",
      "Training batch: 165 in epoch:7, training batch loss:0.4305\n",
      "Training batch: 170 in epoch:7, training batch loss:0.4311\n",
      "Training batch: 175 in epoch:7, training batch loss:0.4357\n",
      "Training batch: 180 in epoch:7, training batch loss:0.4292\n",
      "Training batch: 185 in epoch:7, training batch loss:0.4217\n",
      "Training batch: 190 in epoch:7, training batch loss:0.4321\n",
      "Training batch: 195 in epoch:7, training batch loss:0.4359\n",
      "Training batch: 200 in epoch:7, training batch loss:0.4265\n",
      "Training batch: 205 in epoch:7, training batch loss:0.4333\n",
      "Training batch: 210 in epoch:7, training batch loss:0.4213\n",
      "Training batch: 215 in epoch:7, training batch loss:0.4302\n",
      "Training batch: 220 in epoch:7, training batch loss:0.4204\n",
      "Training batch: 225 in epoch:7, training batch loss:0.4208\n",
      "Training batch: 230 in epoch:7, training batch loss:0.4305\n",
      "Training batch: 235 in epoch:7, training batch loss:0.4264\n",
      "Training batch: 240 in epoch:7, training batch loss:0.4187\n",
      "Training batch: 245 in epoch:7, training batch loss:0.4186\n",
      "Training batch: 250 in epoch:7, training batch loss:0.4315\n",
      "Training batch: 255 in epoch:7, training batch loss:0.4296\n",
      "Training batch: 260 in epoch:7, training batch loss:0.4226\n",
      "Training batch: 265 in epoch:7, training batch loss:0.4320\n",
      "Training batch: 270 in epoch:7, training batch loss:0.4216\n",
      "Training batch: 275 in epoch:7, training batch loss:0.4182\n",
      "Training batch: 280 in epoch:7, training batch loss:0.4263\n",
      "Training batch: 285 in epoch:7, training batch loss:0.4292\n",
      "Training batch: 290 in epoch:7, training batch loss:0.4209\n",
      "Training batch: 295 in epoch:7, training batch loss:0.4194\n",
      "Training batch: 300 in epoch:7, training batch loss:0.4277\n",
      "Training batch: 305 in epoch:7, training batch loss:0.4292\n",
      "Training batch: 310 in epoch:7, training batch loss:0.4242\n",
      "Training batch: 315 in epoch:7, training batch loss:0.4184\n",
      "Training batch: 320 in epoch:7, training batch loss:0.4269\n",
      "Training batch: 325 in epoch:7, training batch loss:0.4311\n",
      "Training batch: 330 in epoch:7, training batch loss:0.4272\n",
      "Training batch: 335 in epoch:7, training batch loss:0.4342\n",
      "Training batch: 340 in epoch:7, training batch loss:0.4314\n",
      "Training batch: 345 in epoch:7, training batch loss:0.4368\n",
      "Training batch: 350 in epoch:7, training batch loss:0.4213\n",
      "Training batch: 355 in epoch:7, training batch loss:0.4247\n",
      "Training batch: 360 in epoch:7, training batch loss:0.4301\n",
      "Training batch: 365 in epoch:7, training batch loss:0.4220\n",
      "Training batch: 370 in epoch:7, training batch loss:0.4215\n",
      "Training batch: 375 in epoch:7, training batch loss:0.4219\n",
      "Training batch: 380 in epoch:7, training batch loss:0.4332\n",
      "Training batch: 385 in epoch:7, training batch loss:0.4327\n",
      "Training batch: 390 in epoch:7, training batch loss:0.4296\n",
      "Training batch: 395 in epoch:7, training batch loss:0.4302\n",
      "Training batch: 400 in epoch:7, training batch loss:0.4210\n",
      "Training batch: 405 in epoch:7, training batch loss:0.4312\n",
      "Training batch: 410 in epoch:7, training batch loss:0.4214\n",
      "Training batch: 415 in epoch:7, training batch loss:0.4288\n",
      "Training batch: 420 in epoch:7, training batch loss:0.4314\n",
      "Training batch: 425 in epoch:7, training batch loss:0.4333\n",
      "Training batch: 430 in epoch:7, training batch loss:0.4227\n",
      "Training batch: 435 in epoch:7, training batch loss:0.4255\n",
      "Training batch: 440 in epoch:7, training batch loss:0.4381\n",
      "Training batch: 445 in epoch:7, training batch loss:0.4258\n",
      "Training batch: 450 in epoch:7, training batch loss:0.4257\n",
      "Training batch: 455 in epoch:7, training batch loss:0.4344\n",
      "Training batch: 460 in epoch:7, training batch loss:0.4323\n",
      "Training batch: 465 in epoch:7, training batch loss:0.4454\n",
      "Training batch: 470 in epoch:7, training batch loss:0.4253\n",
      "Training batch: 475 in epoch:7, training batch loss:0.4312\n",
      "Training batch: 480 in epoch:7, training batch loss:0.4244\n",
      "Training batch: 485 in epoch:7, training batch loss:0.4298\n",
      "Training batch: 490 in epoch:7, training batch loss:0.4297\n",
      "Training batch: 495 in epoch:7, training batch loss:0.4370\n",
      "Training batch: 500 in epoch:7, training batch loss:0.4346\n",
      "Training batch: 505 in epoch:7, training batch loss:0.4324\n",
      "Training batch: 510 in epoch:7, training batch loss:0.4240\n",
      "Training batch: 515 in epoch:7, training batch loss:0.4269\n",
      "Training batch: 520 in epoch:7, training batch loss:0.4317\n",
      "Training batch: 525 in epoch:7, training batch loss:0.4233\n",
      "Training batch: 530 in epoch:7, training batch loss:0.4207\n",
      "Training batch: 535 in epoch:7, training batch loss:0.4266\n",
      "Training batch: 540 in epoch:7, training batch loss:0.4205\n",
      "Training batch: 545 in epoch:7, training batch loss:0.4305\n",
      "Training batch: 550 in epoch:7, training batch loss:0.4187\n",
      "Training batch: 555 in epoch:7, training batch loss:0.4371\n",
      "Training batch: 560 in epoch:7, training batch loss:0.4465\n",
      "Training batch: 565 in epoch:7, training batch loss:0.4312\n",
      "Training batch: 570 in epoch:7, training batch loss:0.4387\n",
      "Training batch: 575 in epoch:7, training batch loss:0.4344\n",
      "2021-04-21 18:20:45 | epoch: 0008/100, training time: 1234.0s, inference time: 272.7s\n",
      "train loss: 0.4288, val_loss: 0.4335\n",
      "val loss decrease from 0.4338 to 0.4335, saving model to ./Model/GMAN/gman4_tjwind(1).pkl\n",
      "Training batch: 5 in epoch:8, training batch loss:0.4310\n",
      "Training batch: 10 in epoch:8, training batch loss:0.4223\n",
      "Training batch: 15 in epoch:8, training batch loss:0.4444\n",
      "Training batch: 20 in epoch:8, training batch loss:0.4266\n",
      "Training batch: 25 in epoch:8, training batch loss:0.4305\n",
      "Training batch: 30 in epoch:8, training batch loss:0.4160\n",
      "Training batch: 35 in epoch:8, training batch loss:0.4306\n",
      "Training batch: 40 in epoch:8, training batch loss:0.4150\n",
      "Training batch: 45 in epoch:8, training batch loss:0.4319\n",
      "Training batch: 50 in epoch:8, training batch loss:0.4188\n",
      "Training batch: 55 in epoch:8, training batch loss:0.4313\n",
      "Training batch: 60 in epoch:8, training batch loss:0.4262\n",
      "Training batch: 65 in epoch:8, training batch loss:0.4274\n",
      "Training batch: 70 in epoch:8, training batch loss:0.4208\n",
      "Training batch: 75 in epoch:8, training batch loss:0.4319\n",
      "Training batch: 80 in epoch:8, training batch loss:0.4261\n",
      "Training batch: 85 in epoch:8, training batch loss:0.4281\n",
      "Training batch: 90 in epoch:8, training batch loss:0.4243\n",
      "Training batch: 95 in epoch:8, training batch loss:0.4251\n",
      "Training batch: 100 in epoch:8, training batch loss:0.4279\n",
      "Training batch: 105 in epoch:8, training batch loss:0.4351\n",
      "Training batch: 110 in epoch:8, training batch loss:0.4275\n",
      "Training batch: 115 in epoch:8, training batch loss:0.4324\n",
      "Training batch: 120 in epoch:8, training batch loss:0.4297\n",
      "Training batch: 125 in epoch:8, training batch loss:0.4322\n",
      "Training batch: 130 in epoch:8, training batch loss:0.4197\n",
      "Training batch: 135 in epoch:8, training batch loss:0.4238\n",
      "Training batch: 140 in epoch:8, training batch loss:0.4251\n",
      "Training batch: 145 in epoch:8, training batch loss:0.4204\n",
      "Training batch: 150 in epoch:8, training batch loss:0.4225\n",
      "Training batch: 155 in epoch:8, training batch loss:0.4236\n",
      "Training batch: 160 in epoch:8, training batch loss:0.4384\n",
      "Training batch: 165 in epoch:8, training batch loss:0.4295\n",
      "Training batch: 170 in epoch:8, training batch loss:0.4305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 175 in epoch:8, training batch loss:0.4263\n",
      "Training batch: 180 in epoch:8, training batch loss:0.4279\n",
      "Training batch: 185 in epoch:8, training batch loss:0.4328\n",
      "Training batch: 190 in epoch:8, training batch loss:0.4264\n",
      "Training batch: 195 in epoch:8, training batch loss:0.4245\n",
      "Training batch: 200 in epoch:8, training batch loss:0.4304\n",
      "Training batch: 205 in epoch:8, training batch loss:0.4292\n",
      "Training batch: 210 in epoch:8, training batch loss:0.4275\n",
      "Training batch: 215 in epoch:8, training batch loss:0.4312\n",
      "Training batch: 220 in epoch:8, training batch loss:0.4291\n",
      "Training batch: 225 in epoch:8, training batch loss:0.4174\n",
      "Training batch: 230 in epoch:8, training batch loss:0.4214\n",
      "Training batch: 235 in epoch:8, training batch loss:0.4278\n",
      "Training batch: 240 in epoch:8, training batch loss:0.4289\n",
      "Training batch: 245 in epoch:8, training batch loss:0.4196\n",
      "Training batch: 250 in epoch:8, training batch loss:0.4198\n",
      "Training batch: 255 in epoch:8, training batch loss:0.4289\n",
      "Training batch: 260 in epoch:8, training batch loss:0.4352\n",
      "Training batch: 265 in epoch:8, training batch loss:0.4226\n",
      "Training batch: 270 in epoch:8, training batch loss:0.4342\n",
      "Training batch: 275 in epoch:8, training batch loss:0.4283\n",
      "Training batch: 280 in epoch:8, training batch loss:0.4395\n",
      "Training batch: 285 in epoch:8, training batch loss:0.4224\n",
      "Training batch: 290 in epoch:8, training batch loss:0.4339\n",
      "Training batch: 295 in epoch:8, training batch loss:0.4330\n",
      "Training batch: 300 in epoch:8, training batch loss:0.4204\n",
      "Training batch: 305 in epoch:8, training batch loss:0.4260\n",
      "Training batch: 310 in epoch:8, training batch loss:0.4334\n",
      "Training batch: 315 in epoch:8, training batch loss:0.4350\n",
      "Training batch: 320 in epoch:8, training batch loss:0.4342\n",
      "Training batch: 325 in epoch:8, training batch loss:0.4373\n",
      "Training batch: 330 in epoch:8, training batch loss:0.4303\n",
      "Training batch: 335 in epoch:8, training batch loss:0.4329\n",
      "Training batch: 340 in epoch:8, training batch loss:0.4293\n",
      "Training batch: 345 in epoch:8, training batch loss:0.4242\n",
      "Training batch: 350 in epoch:8, training batch loss:0.4355\n",
      "Training batch: 355 in epoch:8, training batch loss:0.4268\n",
      "Training batch: 360 in epoch:8, training batch loss:0.4378\n",
      "Training batch: 365 in epoch:8, training batch loss:0.4264\n",
      "Training batch: 370 in epoch:8, training batch loss:0.4295\n",
      "Training batch: 375 in epoch:8, training batch loss:0.4291\n",
      "Training batch: 380 in epoch:8, training batch loss:0.4344\n",
      "Training batch: 385 in epoch:8, training batch loss:0.4251\n",
      "Training batch: 390 in epoch:8, training batch loss:0.4179\n",
      "Training batch: 395 in epoch:8, training batch loss:0.4250\n",
      "Training batch: 400 in epoch:8, training batch loss:0.4258\n",
      "Training batch: 405 in epoch:8, training batch loss:0.4248\n",
      "Training batch: 410 in epoch:8, training batch loss:0.4238\n",
      "Training batch: 415 in epoch:8, training batch loss:0.4382\n",
      "Training batch: 420 in epoch:8, training batch loss:0.4354\n",
      "Training batch: 425 in epoch:8, training batch loss:0.4246\n",
      "Training batch: 430 in epoch:8, training batch loss:0.4272\n",
      "Training batch: 435 in epoch:8, training batch loss:0.4340\n",
      "Training batch: 440 in epoch:8, training batch loss:0.4250\n",
      "Training batch: 445 in epoch:8, training batch loss:0.4205\n",
      "Training batch: 450 in epoch:8, training batch loss:0.4248\n",
      "Training batch: 455 in epoch:8, training batch loss:0.4200\n",
      "Training batch: 460 in epoch:8, training batch loss:0.4198\n",
      "Training batch: 465 in epoch:8, training batch loss:0.4315\n",
      "Training batch: 470 in epoch:8, training batch loss:0.4303\n",
      "Training batch: 475 in epoch:8, training batch loss:0.4250\n",
      "Training batch: 480 in epoch:8, training batch loss:0.4245\n",
      "Training batch: 485 in epoch:8, training batch loss:0.4377\n",
      "Training batch: 490 in epoch:8, training batch loss:0.4254\n",
      "Training batch: 495 in epoch:8, training batch loss:0.4306\n",
      "Training batch: 500 in epoch:8, training batch loss:0.4204\n",
      "Training batch: 505 in epoch:8, training batch loss:0.4395\n",
      "Training batch: 510 in epoch:8, training batch loss:0.4336\n",
      "Training batch: 515 in epoch:8, training batch loss:0.4257\n",
      "Training batch: 520 in epoch:8, training batch loss:0.4398\n",
      "Training batch: 525 in epoch:8, training batch loss:0.4279\n",
      "Training batch: 530 in epoch:8, training batch loss:0.4192\n",
      "Training batch: 535 in epoch:8, training batch loss:0.4262\n",
      "Training batch: 540 in epoch:8, training batch loss:0.4268\n",
      "Training batch: 545 in epoch:8, training batch loss:0.4360\n",
      "Training batch: 550 in epoch:8, training batch loss:0.4282\n",
      "Training batch: 555 in epoch:8, training batch loss:0.4271\n",
      "Training batch: 560 in epoch:8, training batch loss:0.4281\n",
      "Training batch: 565 in epoch:8, training batch loss:0.4323\n",
      "Training batch: 570 in epoch:8, training batch loss:0.4246\n",
      "Training batch: 575 in epoch:8, training batch loss:0.4256\n",
      "2021-04-21 18:46:22 | epoch: 0009/100, training time: 1233.3s, inference time: 303.7s\n",
      "train loss: 0.4281, val_loss: 0.4338\n",
      "Training batch: 5 in epoch:9, training batch loss:0.4421\n",
      "Training batch: 10 in epoch:9, training batch loss:0.4266\n",
      "Training batch: 15 in epoch:9, training batch loss:0.4322\n",
      "Training batch: 20 in epoch:9, training batch loss:0.4269\n",
      "Training batch: 25 in epoch:9, training batch loss:0.4284\n",
      "Training batch: 30 in epoch:9, training batch loss:0.4224\n",
      "Training batch: 35 in epoch:9, training batch loss:0.4310\n",
      "Training batch: 40 in epoch:9, training batch loss:0.4426\n",
      "Training batch: 45 in epoch:9, training batch loss:0.4232\n",
      "Training batch: 50 in epoch:9, training batch loss:0.4116\n",
      "Training batch: 55 in epoch:9, training batch loss:0.4375\n",
      "Training batch: 60 in epoch:9, training batch loss:0.4298\n",
      "Training batch: 65 in epoch:9, training batch loss:0.4309\n",
      "Training batch: 70 in epoch:9, training batch loss:0.4291\n",
      "Training batch: 75 in epoch:9, training batch loss:0.4216\n",
      "Training batch: 80 in epoch:9, training batch loss:0.4282\n",
      "Training batch: 85 in epoch:9, training batch loss:0.4254\n",
      "Training batch: 90 in epoch:9, training batch loss:0.4241\n",
      "Training batch: 95 in epoch:9, training batch loss:0.4311\n",
      "Training batch: 100 in epoch:9, training batch loss:0.4186\n",
      "Training batch: 105 in epoch:9, training batch loss:0.4210\n",
      "Training batch: 110 in epoch:9, training batch loss:0.4193\n",
      "Training batch: 115 in epoch:9, training batch loss:0.4185\n",
      "Training batch: 120 in epoch:9, training batch loss:0.4300\n",
      "Training batch: 125 in epoch:9, training batch loss:0.4200\n",
      "Training batch: 130 in epoch:9, training batch loss:0.4199\n",
      "Training batch: 135 in epoch:9, training batch loss:0.4242\n",
      "Training batch: 140 in epoch:9, training batch loss:0.4276\n",
      "Training batch: 145 in epoch:9, training batch loss:0.4370\n",
      "Training batch: 150 in epoch:9, training batch loss:0.4251\n",
      "Training batch: 155 in epoch:9, training batch loss:0.4191\n",
      "Training batch: 160 in epoch:9, training batch loss:0.4219\n",
      "Training batch: 165 in epoch:9, training batch loss:0.4248\n",
      "Training batch: 170 in epoch:9, training batch loss:0.4316\n",
      "Training batch: 175 in epoch:9, training batch loss:0.4230\n",
      "Training batch: 180 in epoch:9, training batch loss:0.4268\n",
      "Training batch: 185 in epoch:9, training batch loss:0.4248\n",
      "Training batch: 190 in epoch:9, training batch loss:0.4304\n",
      "Training batch: 195 in epoch:9, training batch loss:0.4283\n",
      "Training batch: 200 in epoch:9, training batch loss:0.4309\n",
      "Training batch: 205 in epoch:9, training batch loss:0.4247\n",
      "Training batch: 210 in epoch:9, training batch loss:0.4346\n",
      "Training batch: 215 in epoch:9, training batch loss:0.4304\n",
      "Training batch: 220 in epoch:9, training batch loss:0.4256\n",
      "Training batch: 225 in epoch:9, training batch loss:0.4399\n",
      "Training batch: 230 in epoch:9, training batch loss:0.4296\n",
      "Training batch: 235 in epoch:9, training batch loss:0.4241\n",
      "Training batch: 240 in epoch:9, training batch loss:0.4319\n",
      "Training batch: 245 in epoch:9, training batch loss:0.4343\n",
      "Training batch: 250 in epoch:9, training batch loss:0.4218\n",
      "Training batch: 255 in epoch:9, training batch loss:0.4192\n",
      "Training batch: 260 in epoch:9, training batch loss:0.4212\n",
      "Training batch: 265 in epoch:9, training batch loss:0.4226\n",
      "Training batch: 270 in epoch:9, training batch loss:0.4177\n",
      "Training batch: 275 in epoch:9, training batch loss:0.4278\n",
      "Training batch: 280 in epoch:9, training batch loss:0.4295\n",
      "Training batch: 285 in epoch:9, training batch loss:0.4168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 290 in epoch:9, training batch loss:0.4266\n",
      "Training batch: 295 in epoch:9, training batch loss:0.4385\n",
      "Training batch: 300 in epoch:9, training batch loss:0.4203\n",
      "Training batch: 305 in epoch:9, training batch loss:0.4268\n",
      "Training batch: 310 in epoch:9, training batch loss:0.4237\n",
      "Training batch: 315 in epoch:9, training batch loss:0.4264\n",
      "Training batch: 320 in epoch:9, training batch loss:0.4318\n",
      "Training batch: 325 in epoch:9, training batch loss:0.4382\n",
      "Training batch: 330 in epoch:9, training batch loss:0.4258\n",
      "Training batch: 335 in epoch:9, training batch loss:0.4277\n",
      "Training batch: 340 in epoch:9, training batch loss:0.4346\n",
      "Training batch: 345 in epoch:9, training batch loss:0.4224\n",
      "Training batch: 350 in epoch:9, training batch loss:0.4234\n",
      "Training batch: 355 in epoch:9, training batch loss:0.4326\n",
      "Training batch: 360 in epoch:9, training batch loss:0.4373\n",
      "Training batch: 365 in epoch:9, training batch loss:0.4319\n",
      "Training batch: 370 in epoch:9, training batch loss:0.4305\n",
      "Training batch: 375 in epoch:9, training batch loss:0.4268\n",
      "Training batch: 380 in epoch:9, training batch loss:0.4343\n",
      "Training batch: 385 in epoch:9, training batch loss:0.4339\n",
      "Training batch: 390 in epoch:9, training batch loss:0.4283\n",
      "Training batch: 395 in epoch:9, training batch loss:0.4339\n",
      "Training batch: 400 in epoch:9, training batch loss:0.4251\n",
      "Training batch: 405 in epoch:9, training batch loss:0.4260\n",
      "Training batch: 410 in epoch:9, training batch loss:0.4230\n",
      "Training batch: 415 in epoch:9, training batch loss:0.4230\n",
      "Training batch: 420 in epoch:9, training batch loss:0.4228\n",
      "Training batch: 425 in epoch:9, training batch loss:0.4274\n",
      "Training batch: 430 in epoch:9, training batch loss:0.4117\n",
      "Training batch: 435 in epoch:9, training batch loss:0.4307\n",
      "Training batch: 440 in epoch:9, training batch loss:0.4175\n",
      "Training batch: 445 in epoch:9, training batch loss:0.4332\n",
      "Training batch: 450 in epoch:9, training batch loss:0.4234\n",
      "Training batch: 455 in epoch:9, training batch loss:0.4170\n",
      "Training batch: 460 in epoch:9, training batch loss:0.4328\n",
      "Training batch: 465 in epoch:9, training batch loss:0.4256\n",
      "Training batch: 470 in epoch:9, training batch loss:0.4282\n",
      "Training batch: 475 in epoch:9, training batch loss:0.4254\n",
      "Training batch: 480 in epoch:9, training batch loss:0.4233\n",
      "Training batch: 485 in epoch:9, training batch loss:0.4359\n",
      "Training batch: 490 in epoch:9, training batch loss:0.4341\n",
      "Training batch: 495 in epoch:9, training batch loss:0.4203\n",
      "Training batch: 500 in epoch:9, training batch loss:0.4354\n",
      "Training batch: 505 in epoch:9, training batch loss:0.4260\n",
      "Training batch: 510 in epoch:9, training batch loss:0.4216\n",
      "Training batch: 515 in epoch:9, training batch loss:0.4266\n",
      "Training batch: 520 in epoch:9, training batch loss:0.4330\n",
      "Training batch: 525 in epoch:9, training batch loss:0.4280\n",
      "Training batch: 530 in epoch:9, training batch loss:0.4244\n",
      "Training batch: 535 in epoch:9, training batch loss:0.4354\n",
      "Training batch: 540 in epoch:9, training batch loss:0.4263\n",
      "Training batch: 545 in epoch:9, training batch loss:0.4358\n",
      "Training batch: 550 in epoch:9, training batch loss:0.4316\n",
      "Training batch: 555 in epoch:9, training batch loss:0.4183\n",
      "Training batch: 560 in epoch:9, training batch loss:0.4307\n",
      "Training batch: 565 in epoch:9, training batch loss:0.4264\n",
      "Training batch: 570 in epoch:9, training batch loss:0.4341\n",
      "Training batch: 575 in epoch:9, training batch loss:0.4331\n",
      "2021-04-21 19:11:57 | epoch: 0010/100, training time: 1231.4s, inference time: 303.4s\n",
      "train loss: 0.4276, val_loss: 0.4336\n",
      "Training batch: 5 in epoch:10, training batch loss:0.4307\n",
      "Training batch: 10 in epoch:10, training batch loss:0.4313\n",
      "Training batch: 15 in epoch:10, training batch loss:0.4200\n",
      "Training batch: 20 in epoch:10, training batch loss:0.4210\n",
      "Training batch: 25 in epoch:10, training batch loss:0.4375\n",
      "Training batch: 30 in epoch:10, training batch loss:0.4265\n",
      "Training batch: 35 in epoch:10, training batch loss:0.4221\n",
      "Training batch: 40 in epoch:10, training batch loss:0.4281\n",
      "Training batch: 45 in epoch:10, training batch loss:0.4284\n",
      "Training batch: 50 in epoch:10, training batch loss:0.4355\n",
      "Training batch: 55 in epoch:10, training batch loss:0.4211\n",
      "Training batch: 60 in epoch:10, training batch loss:0.4266\n",
      "Training batch: 65 in epoch:10, training batch loss:0.4377\n",
      "Training batch: 70 in epoch:10, training batch loss:0.4176\n",
      "Training batch: 75 in epoch:10, training batch loss:0.4310\n",
      "Training batch: 80 in epoch:10, training batch loss:0.4138\n",
      "Training batch: 85 in epoch:10, training batch loss:0.4349\n",
      "Training batch: 90 in epoch:10, training batch loss:0.4203\n",
      "Training batch: 95 in epoch:10, training batch loss:0.4242\n",
      "Training batch: 100 in epoch:10, training batch loss:0.4237\n",
      "Training batch: 105 in epoch:10, training batch loss:0.4268\n",
      "Training batch: 110 in epoch:10, training batch loss:0.4190\n",
      "Training batch: 115 in epoch:10, training batch loss:0.4266\n",
      "Training batch: 120 in epoch:10, training batch loss:0.4262\n",
      "Training batch: 125 in epoch:10, training batch loss:0.4255\n",
      "Training batch: 130 in epoch:10, training batch loss:0.4253\n",
      "Training batch: 135 in epoch:10, training batch loss:0.4279\n",
      "Training batch: 140 in epoch:10, training batch loss:0.4309\n",
      "Training batch: 145 in epoch:10, training batch loss:0.4252\n",
      "Training batch: 150 in epoch:10, training batch loss:0.4227\n",
      "Training batch: 155 in epoch:10, training batch loss:0.4307\n",
      "Training batch: 160 in epoch:10, training batch loss:0.4195\n",
      "Training batch: 165 in epoch:10, training batch loss:0.4245\n",
      "Training batch: 170 in epoch:10, training batch loss:0.4172\n",
      "Training batch: 175 in epoch:10, training batch loss:0.4222\n",
      "Training batch: 180 in epoch:10, training batch loss:0.4343\n",
      "Training batch: 185 in epoch:10, training batch loss:0.4209\n",
      "Training batch: 190 in epoch:10, training batch loss:0.4312\n",
      "Training batch: 195 in epoch:10, training batch loss:0.4187\n",
      "Training batch: 200 in epoch:10, training batch loss:0.4231\n",
      "Training batch: 205 in epoch:10, training batch loss:0.4206\n",
      "Training batch: 210 in epoch:10, training batch loss:0.4258\n",
      "Training batch: 215 in epoch:10, training batch loss:0.4350\n",
      "Training batch: 220 in epoch:10, training batch loss:0.4272\n",
      "Training batch: 225 in epoch:10, training batch loss:0.4373\n",
      "Training batch: 230 in epoch:10, training batch loss:0.4315\n",
      "Training batch: 235 in epoch:10, training batch loss:0.4201\n",
      "Training batch: 240 in epoch:10, training batch loss:0.4178\n",
      "Training batch: 245 in epoch:10, training batch loss:0.4305\n",
      "Training batch: 250 in epoch:10, training batch loss:0.4275\n",
      "Training batch: 255 in epoch:10, training batch loss:0.4284\n",
      "Training batch: 260 in epoch:10, training batch loss:0.4297\n",
      "Training batch: 265 in epoch:10, training batch loss:0.4232\n",
      "Training batch: 270 in epoch:10, training batch loss:0.4232\n",
      "Training batch: 275 in epoch:10, training batch loss:0.4317\n",
      "Training batch: 280 in epoch:10, training batch loss:0.4314\n",
      "Training batch: 285 in epoch:10, training batch loss:0.4271\n",
      "Training batch: 290 in epoch:10, training batch loss:0.4377\n",
      "Training batch: 295 in epoch:10, training batch loss:0.4251\n",
      "Training batch: 300 in epoch:10, training batch loss:0.4193\n",
      "Training batch: 305 in epoch:10, training batch loss:0.4218\n",
      "Training batch: 310 in epoch:10, training batch loss:0.4292\n",
      "Training batch: 315 in epoch:10, training batch loss:0.4269\n",
      "Training batch: 320 in epoch:10, training batch loss:0.4206\n",
      "Training batch: 325 in epoch:10, training batch loss:0.4250\n",
      "Training batch: 330 in epoch:10, training batch loss:0.4156\n",
      "Training batch: 335 in epoch:10, training batch loss:0.4326\n",
      "Training batch: 340 in epoch:10, training batch loss:0.4349\n",
      "Training batch: 345 in epoch:10, training batch loss:0.4244\n",
      "Training batch: 350 in epoch:10, training batch loss:0.4309\n",
      "Training batch: 355 in epoch:10, training batch loss:0.4409\n",
      "Training batch: 360 in epoch:10, training batch loss:0.4278\n",
      "Training batch: 365 in epoch:10, training batch loss:0.4215\n",
      "Training batch: 370 in epoch:10, training batch loss:0.4426\n",
      "Training batch: 375 in epoch:10, training batch loss:0.4249\n",
      "Training batch: 380 in epoch:10, training batch loss:0.4253\n",
      "Training batch: 385 in epoch:10, training batch loss:0.4227\n",
      "Training batch: 390 in epoch:10, training batch loss:0.4228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 395 in epoch:10, training batch loss:0.4213\n",
      "Training batch: 400 in epoch:10, training batch loss:0.4232\n",
      "Training batch: 405 in epoch:10, training batch loss:0.4300\n",
      "Training batch: 410 in epoch:10, training batch loss:0.4286\n",
      "Training batch: 415 in epoch:10, training batch loss:0.4147\n",
      "Training batch: 420 in epoch:10, training batch loss:0.4228\n",
      "Training batch: 425 in epoch:10, training batch loss:0.4274\n",
      "Training batch: 430 in epoch:10, training batch loss:0.4388\n",
      "Training batch: 435 in epoch:10, training batch loss:0.4228\n",
      "Training batch: 440 in epoch:10, training batch loss:0.4295\n",
      "Training batch: 445 in epoch:10, training batch loss:0.4269\n",
      "Training batch: 450 in epoch:10, training batch loss:0.4180\n",
      "Training batch: 455 in epoch:10, training batch loss:0.4197\n",
      "Training batch: 460 in epoch:10, training batch loss:0.4244\n",
      "Training batch: 465 in epoch:10, training batch loss:0.4290\n",
      "Training batch: 470 in epoch:10, training batch loss:0.4198\n",
      "Training batch: 475 in epoch:10, training batch loss:0.4201\n",
      "Training batch: 480 in epoch:10, training batch loss:0.4300\n",
      "Training batch: 485 in epoch:10, training batch loss:0.4228\n",
      "Training batch: 490 in epoch:10, training batch loss:0.4330\n",
      "Training batch: 495 in epoch:10, training batch loss:0.4198\n",
      "Training batch: 500 in epoch:10, training batch loss:0.4261\n",
      "Training batch: 505 in epoch:10, training batch loss:0.4354\n",
      "Training batch: 510 in epoch:10, training batch loss:0.4271\n",
      "Training batch: 515 in epoch:10, training batch loss:0.4286\n",
      "Training batch: 520 in epoch:10, training batch loss:0.4300\n",
      "Training batch: 525 in epoch:10, training batch loss:0.4228\n",
      "Training batch: 530 in epoch:10, training batch loss:0.4355\n",
      "Training batch: 535 in epoch:10, training batch loss:0.4190\n",
      "Training batch: 540 in epoch:10, training batch loss:0.4232\n",
      "Training batch: 545 in epoch:10, training batch loss:0.4353\n",
      "Training batch: 550 in epoch:10, training batch loss:0.4323\n",
      "Training batch: 555 in epoch:10, training batch loss:0.4295\n",
      "Training batch: 560 in epoch:10, training batch loss:0.4243\n",
      "Training batch: 565 in epoch:10, training batch loss:0.4086\n",
      "Training batch: 570 in epoch:10, training batch loss:0.4260\n",
      "Training batch: 575 in epoch:10, training batch loss:0.4247\n",
      "2021-04-21 19:37:30 | epoch: 0011/100, training time: 1229.7s, inference time: 303.3s\n",
      "train loss: 0.4267, val_loss: 0.4336\n",
      "Training batch: 5 in epoch:11, training batch loss:0.4317\n",
      "Training batch: 10 in epoch:11, training batch loss:0.4247\n",
      "Training batch: 15 in epoch:11, training batch loss:0.4283\n",
      "Training batch: 20 in epoch:11, training batch loss:0.4252\n",
      "Training batch: 25 in epoch:11, training batch loss:0.4307\n",
      "Training batch: 30 in epoch:11, training batch loss:0.4158\n",
      "Training batch: 35 in epoch:11, training batch loss:0.4296\n",
      "Training batch: 40 in epoch:11, training batch loss:0.4290\n",
      "Training batch: 45 in epoch:11, training batch loss:0.4331\n",
      "Training batch: 50 in epoch:11, training batch loss:0.4312\n",
      "Training batch: 55 in epoch:11, training batch loss:0.4224\n",
      "Training batch: 60 in epoch:11, training batch loss:0.4168\n",
      "Training batch: 65 in epoch:11, training batch loss:0.4244\n",
      "Training batch: 70 in epoch:11, training batch loss:0.4235\n",
      "Training batch: 75 in epoch:11, training batch loss:0.4145\n",
      "Training batch: 80 in epoch:11, training batch loss:0.4365\n",
      "Training batch: 85 in epoch:11, training batch loss:0.4232\n",
      "Training batch: 90 in epoch:11, training batch loss:0.4194\n",
      "Training batch: 95 in epoch:11, training batch loss:0.4282\n",
      "Training batch: 100 in epoch:11, training batch loss:0.4302\n",
      "Training batch: 105 in epoch:11, training batch loss:0.4303\n",
      "Training batch: 110 in epoch:11, training batch loss:0.4298\n",
      "Training batch: 115 in epoch:11, training batch loss:0.4220\n",
      "Training batch: 120 in epoch:11, training batch loss:0.4384\n",
      "Training batch: 125 in epoch:11, training batch loss:0.4164\n",
      "Training batch: 130 in epoch:11, training batch loss:0.4288\n",
      "Training batch: 135 in epoch:11, training batch loss:0.4347\n",
      "Training batch: 140 in epoch:11, training batch loss:0.4312\n",
      "Training batch: 145 in epoch:11, training batch loss:0.4323\n",
      "Training batch: 150 in epoch:11, training batch loss:0.4354\n",
      "Training batch: 155 in epoch:11, training batch loss:0.4217\n",
      "Training batch: 160 in epoch:11, training batch loss:0.4238\n",
      "Training batch: 165 in epoch:11, training batch loss:0.4265\n",
      "Training batch: 170 in epoch:11, training batch loss:0.4235\n",
      "Training batch: 175 in epoch:11, training batch loss:0.4328\n",
      "Training batch: 180 in epoch:11, training batch loss:0.4209\n",
      "Training batch: 185 in epoch:11, training batch loss:0.4331\n",
      "Training batch: 190 in epoch:11, training batch loss:0.4314\n",
      "Training batch: 195 in epoch:11, training batch loss:0.4237\n",
      "Training batch: 200 in epoch:11, training batch loss:0.4299\n",
      "Training batch: 205 in epoch:11, training batch loss:0.4146\n",
      "Training batch: 210 in epoch:11, training batch loss:0.4225\n",
      "Training batch: 215 in epoch:11, training batch loss:0.4230\n",
      "Training batch: 220 in epoch:11, training batch loss:0.4319\n",
      "Training batch: 225 in epoch:11, training batch loss:0.4229\n",
      "Training batch: 230 in epoch:11, training batch loss:0.4269\n",
      "Training batch: 235 in epoch:11, training batch loss:0.4474\n",
      "Training batch: 240 in epoch:11, training batch loss:0.4278\n",
      "Training batch: 245 in epoch:11, training batch loss:0.4237\n",
      "Training batch: 250 in epoch:11, training batch loss:0.4191\n",
      "Training batch: 255 in epoch:11, training batch loss:0.4215\n",
      "Training batch: 260 in epoch:11, training batch loss:0.4220\n",
      "Training batch: 265 in epoch:11, training batch loss:0.4211\n",
      "Training batch: 270 in epoch:11, training batch loss:0.4243\n",
      "Training batch: 275 in epoch:11, training batch loss:0.4265\n",
      "Training batch: 280 in epoch:11, training batch loss:0.4250\n",
      "Training batch: 285 in epoch:11, training batch loss:0.4281\n",
      "Training batch: 290 in epoch:11, training batch loss:0.4284\n",
      "Training batch: 295 in epoch:11, training batch loss:0.4195\n",
      "Training batch: 300 in epoch:11, training batch loss:0.4288\n",
      "Training batch: 305 in epoch:11, training batch loss:0.4165\n",
      "Training batch: 310 in epoch:11, training batch loss:0.4187\n",
      "Training batch: 315 in epoch:11, training batch loss:0.4227\n",
      "Training batch: 320 in epoch:11, training batch loss:0.4326\n",
      "Training batch: 325 in epoch:11, training batch loss:0.4280\n",
      "Training batch: 330 in epoch:11, training batch loss:0.4307\n",
      "Training batch: 335 in epoch:11, training batch loss:0.4162\n",
      "Training batch: 340 in epoch:11, training batch loss:0.4275\n",
      "Training batch: 345 in epoch:11, training batch loss:0.4316\n",
      "Training batch: 350 in epoch:11, training batch loss:0.4292\n",
      "Training batch: 355 in epoch:11, training batch loss:0.4270\n",
      "Training batch: 360 in epoch:11, training batch loss:0.4247\n",
      "Training batch: 365 in epoch:11, training batch loss:0.4316\n",
      "Training batch: 370 in epoch:11, training batch loss:0.4282\n",
      "Training batch: 375 in epoch:11, training batch loss:0.4200\n",
      "Training batch: 380 in epoch:11, training batch loss:0.4372\n",
      "Training batch: 385 in epoch:11, training batch loss:0.4207\n",
      "Training batch: 390 in epoch:11, training batch loss:0.4161\n",
      "Training batch: 395 in epoch:11, training batch loss:0.4215\n",
      "Training batch: 400 in epoch:11, training batch loss:0.4272\n",
      "Training batch: 405 in epoch:11, training batch loss:0.4373\n",
      "Training batch: 410 in epoch:11, training batch loss:0.4217\n",
      "Training batch: 415 in epoch:11, training batch loss:0.4290\n",
      "Training batch: 420 in epoch:11, training batch loss:0.4347\n",
      "Training batch: 425 in epoch:11, training batch loss:0.4135\n",
      "Training batch: 430 in epoch:11, training batch loss:0.4300\n",
      "Training batch: 435 in epoch:11, training batch loss:0.4250\n",
      "Training batch: 440 in epoch:11, training batch loss:0.4414\n",
      "Training batch: 445 in epoch:11, training batch loss:0.4203\n",
      "Training batch: 450 in epoch:11, training batch loss:0.4187\n",
      "Training batch: 455 in epoch:11, training batch loss:0.4359\n",
      "Training batch: 460 in epoch:11, training batch loss:0.4258\n",
      "Training batch: 465 in epoch:11, training batch loss:0.4216\n",
      "Training batch: 470 in epoch:11, training batch loss:0.4221\n",
      "Training batch: 475 in epoch:11, training batch loss:0.4273\n",
      "Training batch: 480 in epoch:11, training batch loss:0.4237\n",
      "Training batch: 485 in epoch:11, training batch loss:0.4346\n",
      "Training batch: 490 in epoch:11, training batch loss:0.4303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 495 in epoch:11, training batch loss:0.4320\n",
      "Training batch: 500 in epoch:11, training batch loss:0.4231\n",
      "Training batch: 505 in epoch:11, training batch loss:0.4252\n",
      "Training batch: 510 in epoch:11, training batch loss:0.4157\n",
      "Training batch: 515 in epoch:11, training batch loss:0.4244\n",
      "Training batch: 520 in epoch:11, training batch loss:0.4333\n",
      "Training batch: 525 in epoch:11, training batch loss:0.4208\n",
      "Training batch: 530 in epoch:11, training batch loss:0.4194\n",
      "Training batch: 535 in epoch:11, training batch loss:0.4247\n",
      "Training batch: 540 in epoch:11, training batch loss:0.4366\n",
      "Training batch: 545 in epoch:11, training batch loss:0.4263\n",
      "Training batch: 550 in epoch:11, training batch loss:0.4284\n",
      "Training batch: 555 in epoch:11, training batch loss:0.4262\n",
      "Training batch: 560 in epoch:11, training batch loss:0.4206\n",
      "Training batch: 565 in epoch:11, training batch loss:0.4313\n",
      "Training batch: 570 in epoch:11, training batch loss:0.4308\n",
      "Training batch: 575 in epoch:11, training batch loss:0.4336\n",
      "2021-04-21 20:02:59 | epoch: 0012/100, training time: 1233.5s, inference time: 295.8s\n",
      "train loss: 0.4262, val_loss: 0.4344\n",
      "Training batch: 5 in epoch:12, training batch loss:0.4206\n",
      "Training batch: 10 in epoch:12, training batch loss:0.4102\n",
      "Training batch: 15 in epoch:12, training batch loss:0.4205\n",
      "Training batch: 20 in epoch:12, training batch loss:0.4262\n",
      "Training batch: 25 in epoch:12, training batch loss:0.4241\n",
      "Training batch: 30 in epoch:12, training batch loss:0.4202\n",
      "Training batch: 35 in epoch:12, training batch loss:0.4127\n",
      "Training batch: 40 in epoch:12, training batch loss:0.4184\n",
      "Training batch: 45 in epoch:12, training batch loss:0.4251\n",
      "Training batch: 50 in epoch:12, training batch loss:0.4298\n",
      "Training batch: 55 in epoch:12, training batch loss:0.4492\n",
      "Training batch: 60 in epoch:12, training batch loss:0.4230\n",
      "Training batch: 65 in epoch:12, training batch loss:0.4264\n",
      "Training batch: 70 in epoch:12, training batch loss:0.4270\n",
      "Training batch: 75 in epoch:12, training batch loss:0.4273\n",
      "Training batch: 80 in epoch:12, training batch loss:0.4264\n",
      "Training batch: 85 in epoch:12, training batch loss:0.4383\n",
      "Training batch: 90 in epoch:12, training batch loss:0.4280\n",
      "Training batch: 95 in epoch:12, training batch loss:0.4306\n",
      "Training batch: 100 in epoch:12, training batch loss:0.4277\n",
      "Training batch: 105 in epoch:12, training batch loss:0.4218\n",
      "Training batch: 110 in epoch:12, training batch loss:0.4325\n",
      "Training batch: 115 in epoch:12, training batch loss:0.4238\n",
      "Training batch: 120 in epoch:12, training batch loss:0.4285\n",
      "Training batch: 125 in epoch:12, training batch loss:0.4225\n",
      "Training batch: 130 in epoch:12, training batch loss:0.4236\n",
      "Training batch: 135 in epoch:12, training batch loss:0.4312\n",
      "Training batch: 140 in epoch:12, training batch loss:0.4257\n",
      "Training batch: 145 in epoch:12, training batch loss:0.4324\n",
      "Training batch: 150 in epoch:12, training batch loss:0.4327\n",
      "Training batch: 155 in epoch:12, training batch loss:0.4403\n",
      "Training batch: 160 in epoch:12, training batch loss:0.4200\n",
      "Training batch: 165 in epoch:12, training batch loss:0.4269\n",
      "Training batch: 170 in epoch:12, training batch loss:0.4175\n",
      "Training batch: 175 in epoch:12, training batch loss:0.4200\n",
      "Training batch: 180 in epoch:12, training batch loss:0.4251\n",
      "Training batch: 185 in epoch:12, training batch loss:0.4307\n",
      "Training batch: 190 in epoch:12, training batch loss:0.4292\n",
      "Training batch: 195 in epoch:12, training batch loss:0.4297\n",
      "Training batch: 200 in epoch:12, training batch loss:0.4259\n",
      "Training batch: 205 in epoch:12, training batch loss:0.4205\n",
      "Training batch: 210 in epoch:12, training batch loss:0.4356\n",
      "Training batch: 215 in epoch:12, training batch loss:0.4285\n",
      "Training batch: 220 in epoch:12, training batch loss:0.4308\n",
      "Training batch: 225 in epoch:12, training batch loss:0.4455\n",
      "Training batch: 230 in epoch:12, training batch loss:0.4270\n",
      "Training batch: 235 in epoch:12, training batch loss:0.4240\n",
      "Training batch: 240 in epoch:12, training batch loss:0.4314\n",
      "Training batch: 245 in epoch:12, training batch loss:0.4171\n",
      "Training batch: 250 in epoch:12, training batch loss:0.4254\n",
      "Training batch: 255 in epoch:12, training batch loss:0.4286\n",
      "Training batch: 260 in epoch:12, training batch loss:0.4380\n",
      "Training batch: 265 in epoch:12, training batch loss:0.4223\n",
      "Training batch: 270 in epoch:12, training batch loss:0.4239\n",
      "Training batch: 275 in epoch:12, training batch loss:0.4452\n",
      "Training batch: 280 in epoch:12, training batch loss:0.4302\n",
      "Training batch: 285 in epoch:12, training batch loss:0.4254\n",
      "Training batch: 290 in epoch:12, training batch loss:0.4238\n",
      "Training batch: 295 in epoch:12, training batch loss:0.4149\n",
      "Training batch: 300 in epoch:12, training batch loss:0.4353\n",
      "Training batch: 305 in epoch:12, training batch loss:0.4263\n",
      "Training batch: 310 in epoch:12, training batch loss:0.4192\n",
      "Training batch: 315 in epoch:12, training batch loss:0.4228\n",
      "Training batch: 320 in epoch:12, training batch loss:0.4212\n",
      "Training batch: 325 in epoch:12, training batch loss:0.4225\n",
      "Training batch: 330 in epoch:12, training batch loss:0.4287\n",
      "Training batch: 335 in epoch:12, training batch loss:0.4364\n",
      "Training batch: 340 in epoch:12, training batch loss:0.4163\n",
      "Training batch: 345 in epoch:12, training batch loss:0.4269\n",
      "Training batch: 350 in epoch:12, training batch loss:0.4235\n",
      "Training batch: 355 in epoch:12, training batch loss:0.4260\n",
      "Training batch: 360 in epoch:12, training batch loss:0.4351\n",
      "Training batch: 365 in epoch:12, training batch loss:0.4329\n",
      "Training batch: 370 in epoch:12, training batch loss:0.4396\n",
      "Training batch: 375 in epoch:12, training batch loss:0.4283\n",
      "Training batch: 380 in epoch:12, training batch loss:0.4255\n",
      "Training batch: 385 in epoch:12, training batch loss:0.4338\n",
      "Training batch: 390 in epoch:12, training batch loss:0.4280\n",
      "Training batch: 395 in epoch:12, training batch loss:0.4220\n",
      "Training batch: 400 in epoch:12, training batch loss:0.4237\n",
      "Training batch: 405 in epoch:12, training batch loss:0.4336\n",
      "Training batch: 410 in epoch:12, training batch loss:0.4322\n",
      "Training batch: 415 in epoch:12, training batch loss:0.4292\n",
      "Training batch: 420 in epoch:12, training batch loss:0.4306\n",
      "Training batch: 425 in epoch:12, training batch loss:0.4343\n",
      "Training batch: 430 in epoch:12, training batch loss:0.4267\n",
      "Training batch: 435 in epoch:12, training batch loss:0.4330\n",
      "Training batch: 440 in epoch:12, training batch loss:0.4265\n",
      "Training batch: 445 in epoch:12, training batch loss:0.4231\n",
      "Training batch: 450 in epoch:12, training batch loss:0.4260\n",
      "Training batch: 455 in epoch:12, training batch loss:0.4309\n",
      "Training batch: 460 in epoch:12, training batch loss:0.4207\n",
      "Training batch: 465 in epoch:12, training batch loss:0.4303\n",
      "Training batch: 470 in epoch:12, training batch loss:0.4279\n",
      "Training batch: 475 in epoch:12, training batch loss:0.4317\n",
      "Training batch: 480 in epoch:12, training batch loss:0.4266\n",
      "Training batch: 485 in epoch:12, training batch loss:0.4346\n",
      "Training batch: 490 in epoch:12, training batch loss:0.4268\n",
      "Training batch: 495 in epoch:12, training batch loss:0.4344\n",
      "Training batch: 500 in epoch:12, training batch loss:0.4275\n",
      "Training batch: 505 in epoch:12, training batch loss:0.4162\n",
      "Training batch: 510 in epoch:12, training batch loss:0.4216\n",
      "Training batch: 515 in epoch:12, training batch loss:0.4128\n",
      "Training batch: 520 in epoch:12, training batch loss:0.4243\n",
      "Training batch: 525 in epoch:12, training batch loss:0.4147\n",
      "Training batch: 530 in epoch:12, training batch loss:0.4318\n",
      "Training batch: 535 in epoch:12, training batch loss:0.4261\n",
      "Training batch: 540 in epoch:12, training batch loss:0.4221\n",
      "Training batch: 545 in epoch:12, training batch loss:0.4399\n",
      "Training batch: 550 in epoch:12, training batch loss:0.4147\n",
      "Training batch: 555 in epoch:12, training batch loss:0.4141\n",
      "Training batch: 560 in epoch:12, training batch loss:0.4262\n",
      "Training batch: 565 in epoch:12, training batch loss:0.4254\n",
      "Training batch: 570 in epoch:12, training batch loss:0.4287\n",
      "Training batch: 575 in epoch:12, training batch loss:0.4361\n",
      "2021-04-21 20:24:05 | epoch: 0013/100, training time: 1220.2s, inference time: 45.3s\n",
      "train loss: 0.4259, val_loss: 0.4340\n",
      "Training batch: 5 in epoch:13, training batch loss:0.4263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 10 in epoch:13, training batch loss:0.4310\n",
      "Training batch: 15 in epoch:13, training batch loss:0.4327\n",
      "Training batch: 20 in epoch:13, training batch loss:0.4302\n",
      "Training batch: 25 in epoch:13, training batch loss:0.4282\n",
      "Training batch: 30 in epoch:13, training batch loss:0.4290\n",
      "Training batch: 35 in epoch:13, training batch loss:0.4213\n",
      "Training batch: 40 in epoch:13, training batch loss:0.4367\n",
      "Training batch: 45 in epoch:13, training batch loss:0.4263\n",
      "Training batch: 50 in epoch:13, training batch loss:0.4113\n",
      "Training batch: 55 in epoch:13, training batch loss:0.4435\n",
      "Training batch: 60 in epoch:13, training batch loss:0.4205\n",
      "Training batch: 65 in epoch:13, training batch loss:0.4209\n",
      "Training batch: 70 in epoch:13, training batch loss:0.4257\n",
      "Training batch: 75 in epoch:13, training batch loss:0.4247\n",
      "Training batch: 80 in epoch:13, training batch loss:0.4264\n",
      "Training batch: 85 in epoch:13, training batch loss:0.4326\n",
      "Training batch: 90 in epoch:13, training batch loss:0.4312\n",
      "Training batch: 95 in epoch:13, training batch loss:0.4156\n",
      "Training batch: 100 in epoch:13, training batch loss:0.4241\n",
      "Training batch: 105 in epoch:13, training batch loss:0.4226\n",
      "Training batch: 110 in epoch:13, training batch loss:0.4274\n",
      "Training batch: 115 in epoch:13, training batch loss:0.4218\n",
      "Training batch: 120 in epoch:13, training batch loss:0.4125\n",
      "Training batch: 125 in epoch:13, training batch loss:0.4243\n",
      "Training batch: 130 in epoch:13, training batch loss:0.4251\n",
      "Training batch: 135 in epoch:13, training batch loss:0.4293\n",
      "Training batch: 140 in epoch:13, training batch loss:0.4247\n",
      "Training batch: 145 in epoch:13, training batch loss:0.4196\n",
      "Training batch: 150 in epoch:13, training batch loss:0.4273\n",
      "Training batch: 155 in epoch:13, training batch loss:0.4198\n",
      "Training batch: 160 in epoch:13, training batch loss:0.4266\n",
      "Training batch: 165 in epoch:13, training batch loss:0.4202\n",
      "Training batch: 170 in epoch:13, training batch loss:0.4211\n",
      "Training batch: 175 in epoch:13, training batch loss:0.4368\n",
      "Training batch: 180 in epoch:13, training batch loss:0.4430\n",
      "Training batch: 185 in epoch:13, training batch loss:0.4220\n",
      "Training batch: 190 in epoch:13, training batch loss:0.4256\n",
      "Training batch: 195 in epoch:13, training batch loss:0.4281\n",
      "Training batch: 200 in epoch:13, training batch loss:0.4288\n",
      "Training batch: 205 in epoch:13, training batch loss:0.4237\n",
      "Training batch: 210 in epoch:13, training batch loss:0.4286\n",
      "Training batch: 215 in epoch:13, training batch loss:0.4165\n",
      "Training batch: 220 in epoch:13, training batch loss:0.4256\n",
      "Training batch: 225 in epoch:13, training batch loss:0.4246\n",
      "Training batch: 230 in epoch:13, training batch loss:0.4190\n",
      "Training batch: 235 in epoch:13, training batch loss:0.4202\n",
      "Training batch: 240 in epoch:13, training batch loss:0.4187\n",
      "Training batch: 245 in epoch:13, training batch loss:0.4191\n",
      "Training batch: 250 in epoch:13, training batch loss:0.4270\n",
      "Training batch: 255 in epoch:13, training batch loss:0.4208\n",
      "Training batch: 260 in epoch:13, training batch loss:0.4162\n",
      "Training batch: 265 in epoch:13, training batch loss:0.4216\n",
      "Training batch: 270 in epoch:13, training batch loss:0.4304\n",
      "Training batch: 275 in epoch:13, training batch loss:0.4332\n",
      "Training batch: 280 in epoch:13, training batch loss:0.4209\n",
      "Training batch: 285 in epoch:13, training batch loss:0.4234\n",
      "Training batch: 290 in epoch:13, training batch loss:0.4186\n",
      "Training batch: 295 in epoch:13, training batch loss:0.4258\n",
      "Training batch: 300 in epoch:13, training batch loss:0.4337\n",
      "Training batch: 305 in epoch:13, training batch loss:0.4207\n",
      "Training batch: 310 in epoch:13, training batch loss:0.4328\n",
      "Training batch: 315 in epoch:13, training batch loss:0.4280\n",
      "Training batch: 320 in epoch:13, training batch loss:0.4147\n",
      "Training batch: 325 in epoch:13, training batch loss:0.4357\n",
      "Training batch: 330 in epoch:13, training batch loss:0.4276\n",
      "Training batch: 335 in epoch:13, training batch loss:0.4264\n",
      "Training batch: 340 in epoch:13, training batch loss:0.4275\n",
      "Training batch: 345 in epoch:13, training batch loss:0.4201\n",
      "Training batch: 350 in epoch:13, training batch loss:0.4287\n",
      "Training batch: 355 in epoch:13, training batch loss:0.4191\n",
      "Training batch: 360 in epoch:13, training batch loss:0.4167\n",
      "Training batch: 365 in epoch:13, training batch loss:0.4171\n",
      "Training batch: 370 in epoch:13, training batch loss:0.4248\n",
      "Training batch: 375 in epoch:13, training batch loss:0.4259\n",
      "Training batch: 380 in epoch:13, training batch loss:0.4206\n",
      "Training batch: 385 in epoch:13, training batch loss:0.4216\n",
      "Training batch: 390 in epoch:13, training batch loss:0.4264\n",
      "Training batch: 395 in epoch:13, training batch loss:0.4210\n",
      "Training batch: 400 in epoch:13, training batch loss:0.4155\n",
      "Training batch: 405 in epoch:13, training batch loss:0.4250\n",
      "Training batch: 410 in epoch:13, training batch loss:0.4353\n",
      "Training batch: 415 in epoch:13, training batch loss:0.4252\n",
      "Training batch: 420 in epoch:13, training batch loss:0.4257\n",
      "Training batch: 425 in epoch:13, training batch loss:0.4209\n",
      "Training batch: 430 in epoch:13, training batch loss:0.4415\n",
      "Training batch: 435 in epoch:13, training batch loss:0.4212\n",
      "Training batch: 440 in epoch:13, training batch loss:0.4173\n",
      "Training batch: 445 in epoch:13, training batch loss:0.4251\n",
      "Training batch: 450 in epoch:13, training batch loss:0.4263\n",
      "Training batch: 455 in epoch:13, training batch loss:0.4235\n",
      "Training batch: 460 in epoch:13, training batch loss:0.4353\n",
      "Training batch: 465 in epoch:13, training batch loss:0.4238\n",
      "Training batch: 470 in epoch:13, training batch loss:0.4215\n",
      "Training batch: 475 in epoch:13, training batch loss:0.4289\n",
      "Training batch: 480 in epoch:13, training batch loss:0.4404\n",
      "Training batch: 485 in epoch:13, training batch loss:0.4242\n",
      "Training batch: 490 in epoch:13, training batch loss:0.4234\n",
      "Training batch: 495 in epoch:13, training batch loss:0.4231\n",
      "Training batch: 500 in epoch:13, training batch loss:0.4273\n",
      "Training batch: 505 in epoch:13, training batch loss:0.4269\n",
      "Training batch: 510 in epoch:13, training batch loss:0.4373\n",
      "Training batch: 515 in epoch:13, training batch loss:0.4276\n",
      "Training batch: 520 in epoch:13, training batch loss:0.4210\n",
      "Training batch: 525 in epoch:13, training batch loss:0.4263\n",
      "Training batch: 530 in epoch:13, training batch loss:0.4359\n",
      "Training batch: 535 in epoch:13, training batch loss:0.4290\n",
      "Training batch: 540 in epoch:13, training batch loss:0.4162\n",
      "Training batch: 545 in epoch:13, training batch loss:0.4109\n",
      "Training batch: 550 in epoch:13, training batch loss:0.4233\n",
      "Training batch: 555 in epoch:13, training batch loss:0.4265\n",
      "Training batch: 560 in epoch:13, training batch loss:0.4320\n",
      "Training batch: 565 in epoch:13, training batch loss:0.4171\n",
      "Training batch: 570 in epoch:13, training batch loss:0.4236\n",
      "Training batch: 575 in epoch:13, training batch loss:0.4231\n",
      "2021-04-21 20:26:58 | epoch: 0014/100, training time: 159.5s, inference time: 14.0s\n",
      "train loss: 0.4256, val_loss: 0.4343\n",
      "Training batch: 5 in epoch:14, training batch loss:0.4222\n",
      "Training batch: 10 in epoch:14, training batch loss:0.4132\n",
      "Training batch: 15 in epoch:14, training batch loss:0.4304\n",
      "Training batch: 20 in epoch:14, training batch loss:0.4245\n",
      "Training batch: 25 in epoch:14, training batch loss:0.4273\n",
      "Training batch: 30 in epoch:14, training batch loss:0.4162\n",
      "Training batch: 35 in epoch:14, training batch loss:0.4187\n",
      "Training batch: 40 in epoch:14, training batch loss:0.4287\n",
      "Training batch: 45 in epoch:14, training batch loss:0.4218\n",
      "Training batch: 50 in epoch:14, training batch loss:0.4207\n",
      "Training batch: 55 in epoch:14, training batch loss:0.4275\n",
      "Training batch: 60 in epoch:14, training batch loss:0.4200\n",
      "Training batch: 65 in epoch:14, training batch loss:0.4203\n",
      "Training batch: 70 in epoch:14, training batch loss:0.4151\n",
      "Training batch: 75 in epoch:14, training batch loss:0.4349\n",
      "Training batch: 80 in epoch:14, training batch loss:0.4119\n",
      "Training batch: 85 in epoch:14, training batch loss:0.4157\n",
      "Training batch: 90 in epoch:14, training batch loss:0.4337\n",
      "Training batch: 95 in epoch:14, training batch loss:0.4296\n",
      "Training batch: 100 in epoch:14, training batch loss:0.4247\n",
      "Training batch: 105 in epoch:14, training batch loss:0.4155\n",
      "Training batch: 110 in epoch:14, training batch loss:0.4240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 115 in epoch:14, training batch loss:0.4277\n",
      "Training batch: 120 in epoch:14, training batch loss:0.4240\n",
      "Training batch: 125 in epoch:14, training batch loss:0.4240\n",
      "Training batch: 130 in epoch:14, training batch loss:0.4225\n",
      "Training batch: 135 in epoch:14, training batch loss:0.4312\n",
      "Training batch: 140 in epoch:14, training batch loss:0.4328\n",
      "Training batch: 145 in epoch:14, training batch loss:0.4276\n",
      "Training batch: 150 in epoch:14, training batch loss:0.4283\n",
      "Training batch: 155 in epoch:14, training batch loss:0.4180\n",
      "Training batch: 160 in epoch:14, training batch loss:0.4244\n",
      "Training batch: 165 in epoch:14, training batch loss:0.4155\n",
      "Training batch: 170 in epoch:14, training batch loss:0.4179\n",
      "Training batch: 175 in epoch:14, training batch loss:0.4305\n",
      "Training batch: 180 in epoch:14, training batch loss:0.4163\n",
      "Training batch: 185 in epoch:14, training batch loss:0.4343\n",
      "Training batch: 190 in epoch:14, training batch loss:0.4178\n",
      "Training batch: 195 in epoch:14, training batch loss:0.4357\n",
      "Training batch: 200 in epoch:14, training batch loss:0.4241\n",
      "Training batch: 205 in epoch:14, training batch loss:0.4263\n",
      "Training batch: 210 in epoch:14, training batch loss:0.4154\n",
      "Training batch: 215 in epoch:14, training batch loss:0.4183\n",
      "Training batch: 220 in epoch:14, training batch loss:0.4222\n",
      "Training batch: 225 in epoch:14, training batch loss:0.4295\n",
      "Training batch: 230 in epoch:14, training batch loss:0.4278\n",
      "Training batch: 235 in epoch:14, training batch loss:0.4251\n",
      "Training batch: 240 in epoch:14, training batch loss:0.4263\n",
      "Training batch: 245 in epoch:14, training batch loss:0.4296\n",
      "Training batch: 250 in epoch:14, training batch loss:0.4369\n",
      "Training batch: 255 in epoch:14, training batch loss:0.4446\n",
      "Training batch: 260 in epoch:14, training batch loss:0.4255\n",
      "Training batch: 265 in epoch:14, training batch loss:0.4260\n",
      "Training batch: 270 in epoch:14, training batch loss:0.4233\n",
      "Training batch: 275 in epoch:14, training batch loss:0.4207\n",
      "Training batch: 280 in epoch:14, training batch loss:0.4230\n",
      "Training batch: 285 in epoch:14, training batch loss:0.4279\n",
      "Training batch: 290 in epoch:14, training batch loss:0.4323\n",
      "Training batch: 295 in epoch:14, training batch loss:0.4340\n",
      "Training batch: 300 in epoch:14, training batch loss:0.4313\n",
      "Training batch: 305 in epoch:14, training batch loss:0.4214\n",
      "Training batch: 310 in epoch:14, training batch loss:0.4246\n",
      "Training batch: 315 in epoch:14, training batch loss:0.4280\n",
      "Training batch: 320 in epoch:14, training batch loss:0.4253\n",
      "Training batch: 325 in epoch:14, training batch loss:0.4211\n",
      "Training batch: 330 in epoch:14, training batch loss:0.4305\n",
      "Training batch: 335 in epoch:14, training batch loss:0.4253\n",
      "Training batch: 340 in epoch:14, training batch loss:0.4152\n",
      "Training batch: 345 in epoch:14, training batch loss:0.4232\n",
      "Training batch: 350 in epoch:14, training batch loss:0.4266\n",
      "Training batch: 355 in epoch:14, training batch loss:0.4199\n",
      "Training batch: 360 in epoch:14, training batch loss:0.4197\n",
      "Training batch: 365 in epoch:14, training batch loss:0.4345\n",
      "Training batch: 370 in epoch:14, training batch loss:0.4289\n"
     ]
    }
   ],
   "source": [
    "log_file = './Dataset/Wind/log(tjWind)'\n",
    "log = open(log_file, 'w')\n",
    "log_string(log, '**** training model ****')\n",
    "num_val = valX.shape[0]\n",
    "num_train = trainX.shape[0]\n",
    "# num_train = 800\n",
    "num_test = testX.shape[0]\n",
    "train_num_batch = math.ceil(num_train / batch_size)\n",
    "val_num_batch = math.ceil(num_val / batch_size)\n",
    "test_num_batch = math.ceil(num_test / batch_size)\n",
    "wait = 0\n",
    "val_loss_min = float('inf')\n",
    "best_model_wts = None\n",
    "train_total_loss = []\n",
    "val_total_loss = []\n",
    "# shuffle\n",
    "SE = SE.to(device)\n",
    "# model = torch.load(model_file)\n",
    "for epoch in range(max_epochs):    \n",
    "    if wait >= patience:\n",
    "        log_string(log, f'early stop at epoch: {epoch:04d}')\n",
    "        break\n",
    "    permutation = torch.randperm(num_train)\n",
    "    trainX = trainX[permutation]\n",
    "    trainTE = trainTE[permutation]\n",
    "    trainY = trainY[permutation]\n",
    "    # train\n",
    "    start_train = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(train_num_batch):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(num_train, (batch_idx + 1) * batch_size)\n",
    "        X = trainX[start_idx: end_idx].to(device)\n",
    "        TE = trainTE[start_idx: end_idx].to(device)\n",
    "        label = trainY[start_idx: end_idx].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X, TE)\n",
    "        pred = pred * std + mean\n",
    "        loss_batch = loss_criterion(pred, label)\n",
    "        train_loss += float(loss_batch) * (end_idx - start_idx)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        if (batch_idx+1) % 5 == 0:\n",
    "            print(f'Training batch: {batch_idx+1} in epoch:{epoch}, training batch loss:{loss_batch:.4f}')\n",
    "        del X, TE, label, pred, loss_batch\n",
    "    train_loss /= num_train\n",
    "    train_total_loss.append(train_loss)\n",
    "    end_train = time.time()\n",
    "\n",
    "    # val loss\n",
    "    start_val = time.time()\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(val_num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_val, (batch_idx + 1) * batch_size)\n",
    "            X = valX[start_idx: end_idx].to(device)\n",
    "            TE = valTE[start_idx: end_idx].to(device)\n",
    "            label = valY[start_idx: end_idx].to(device)\n",
    "            pred = model(X, TE)\n",
    "            pred = pred * std + mean\n",
    "            loss_batch = loss_criterion(pred, label)\n",
    "            val_loss += loss_batch * (end_idx - start_idx)\n",
    "            del X, TE, label, pred, loss_batch\n",
    "    val_loss /= num_val\n",
    "    val_total_loss.append(val_loss)\n",
    "    end_val = time.time()\n",
    "    log_string(\n",
    "        log,\n",
    "        '%s | epoch: %04d/%d, training time: %.1fs, inference time: %.1fs' %\n",
    "        (datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch + 1,\n",
    "         max_epochs, end_train - start_train, end_val - start_val))\n",
    "    log_string(\n",
    "        log, f'train loss: {train_loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "    if val_loss <= val_loss_min:\n",
    "        log_string(\n",
    "            log,\n",
    "            f'val loss decrease from {val_loss_min:.4f} to {val_loss:.4f}, saving model to {model_file}')\n",
    "        wait = 0\n",
    "        val_loss_min = val_loss\n",
    "        best_model_wts = model.state_dict()\n",
    "        torch.save(model, model_file)\n",
    "    else:\n",
    "        wait += 1\n",
    "    scheduler.step()\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model, model_file)\n",
    "log_string(log, f'Training and validation are completed, and model has been stored as {model_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4343, device='cuda:0')\n",
      "tensor(0.5602, device='cuda:0')\n",
      "tensor(0.1270, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model = torch.load(model_file)\n",
    "num_test = testX.shape[0]\n",
    "test_num_batch = math.ceil(num_test / batch_size)\n",
    "testPred = []\n",
    "start_test = time.time()\n",
    "for batch_idx in range(test_num_batch):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(num_test, (batch_idx + 1) * batch_size)\n",
    "    X = testX[start_idx: end_idx].to(device)\n",
    "    TE = testTE[start_idx: end_idx].to(device)\n",
    "    pred_batch = model(X, TE)\n",
    "    testPred.append(pred_batch.detach().clone())\n",
    "    del X, TE, pred_batch\n",
    "testPred = torch.cat(testPred, axis=0)\n",
    "testPred = testPred* std + mean\n",
    "test_mae, test_rmse, test_mape = metric(testPred, testY.to(device))\n",
    "print(test_mae)\n",
    "print(test_rmse)\n",
    "print(test_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEvCAYAAABhSUTPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testPred = testPred.cpu().numpy()\n",
    "# testY = testY.cpu().numpy()\n",
    "testPred = testPred.reshape((-1))\n",
    "testY = testY.reshape((-1))\n",
    "data_range = np.arange(100,150)\n",
    "plt.figure(figsize =(10,5))\n",
    "plt.plot(testPred[data_range],color = 'y')\n",
    "plt.plot(testY[data_range],color ='b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8386ff1b1a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlog_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'**** testing model ****'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlog_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loading model from %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "model= torch.load(model_file)\n",
    "num_train, _, num_vertex = trainX.shape\n",
    "num_val = valX.shape[0]\n",
    "num_test = testX.shape[0]\n",
    "train_num_batch = math.ceil(num_train / batch_size)\n",
    "val_num_batch = math.ceil(num_val / batch_size)\n",
    "test_num_batch = math.ceil(num_test / batch_size)\n",
    "\n",
    "# test model\n",
    "log_string(log, '**** testing model ****')\n",
    "log_string(log, 'loading model from %s' % model_file)\n",
    "model = torch.load(model_file)\n",
    "log_string(log, 'model restored!')\n",
    "log_string(log, 'evaluating...')\n",
    "\n",
    "with torch.no_grad():\n",
    "    trainPred = []\n",
    "    for batch_idx in range(train_num_batch):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(num_train, (batch_idx + 1) * batch_size)\n",
    "        X = trainX[start_idx: end_idx]\n",
    "        TE = trainTE[start_idx: end_idx]\n",
    "        pred_batch = model(X, TE)\n",
    "        trainPred.append(pred_batch.detach().clone())\n",
    "        del X, TE, pred_batch\n",
    "    trainPred = torch.from_numpy(np.concatenate(trainPred, axis=0))\n",
    "    trainPred = trainPred * std + mean\n",
    "\n",
    "    valPred = []\n",
    "    for batch_idx in range(val_num_batch):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(num_val, (batch_idx + 1) * batch_size)\n",
    "        X = valX[start_idx: end_idx]\n",
    "        TE = valTE[start_idx: end_idx]\n",
    "        pred_batch = model(X, TE)\n",
    "        valPred.append(pred_batch.detach().clone())\n",
    "        del X, TE, pred_batch\n",
    "    valPred = torch.from_numpy(np.concatenate(valPred, axis=0))\n",
    "    valPred = valPred * std + mean\n",
    "\n",
    "    testPred = []\n",
    "    start_test = time.time()\n",
    "    for batch_idx in range(test_num_batch):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(num_test, (batch_idx + 1) * batch_size)\n",
    "        X = testX[start_idx: end_idx]\n",
    "        TE = testTE[start_idx: end_idx]\n",
    "        pred_batch = model(X, TE)\n",
    "        testPred.append(pred_batch.detach().clone())\n",
    "        del X, TE, pred_batch\n",
    "    testPred = torch.from_numpy(np.concatenate(testPred, axis=0))\n",
    "    testPred = testPred* std + mean\n",
    "end_test = time.time()\n",
    "train_mae, train_rmse, train_mape = metric(trainPred, trainY)\n",
    "val_mae, val_rmse, val_mape = metric(valPred, valY)\n",
    "test_mae, test_rmse, test_mape = metric(testPred, testY)\n",
    "log_string(log, 'testing time: %.1fs' % (end_test - start_test))\n",
    "log_string(log, '                MAE\\t\\tRMSE\\t\\tMAPE')\n",
    "log_string(log, 'train            %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "           (train_mae, train_rmse, train_mape * 100))\n",
    "log_string(log, 'val              %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "           (val_mae, val_rmse, val_mape * 100))\n",
    "log_string(log, 'test             %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "           (test_mae, test_rmse, test_mape * 100))\n",
    "log_string(log, 'performance in each prediction step')\n",
    "MAE, RMSE, MAPE = [], [], []\n",
    "for step in range(num_pred):\n",
    "    mae, rmse, mape = metric(testPred[:, step], testY[:, step])\n",
    "    MAE.append(mae)\n",
    "    RMSE.append(rmse)\n",
    "    MAPE.append(mape)\n",
    "    log_string(log, 'step: %02d         %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "               (step + 1, mae, rmse, mape * 100))\n",
    "average_mae = np.mean(MAE)\n",
    "average_rmse = np.mean(RMSE)\n",
    "average_mape = np.mean(MAPE)\n",
    "log_string(\n",
    "    log, 'average:         %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "         (average_mae, average_rmse, average_mape * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-lc]",
   "language": "python",
   "name": "conda-env-.conda-lc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
